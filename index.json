[{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"EveryoneCook Architecture EveryoneCook is a modern social cooking platform built entirely on AWS serverless technologies. The architecture follows best practices for scalability, security, and cost optimization, with a focus on Vietnamese ingredient support and AI-powered recipe suggestions.\nKey Components Frontend: Next.js 15 application with Flowbite React components hosted on AWS Amplify Backend: Serverless API using API Gateway with API Router pattern and 6 Lambda modules Database: DynamoDB Single Table Design with username-based PK and 5 GSI indexes Search: OpenSearch with Vietnamese analyzer for advanced recipe and post search Storage: S3 buckets with Intelligent-Tiering and CloudFront CDN with OAC Authentication: Cognito User Pool with Lambda triggers for custom flows Email: SES with DKIM signing for transactional emails AI Integration: Amazon Bedrock (Claude 3.5 Sonnet v2) with dictionary-first translation Security: WAF for API Gateway, Shield Standard for CloudFront, KMS encryption Monitoring: CloudWatch dashboards, alarms, and X-Ray distributed tracing Architecture Diagram ┌─────────────────────────────────┐ │ Route 53 DNS │ │ everyonecook.cloud │ └──────┬──────────────┬───────────┘ │ │ ┌──────────▼──────┐ ┌───▼──────────────┐ │ CloudFront CDN │ │ Amplify Hosting │ │ cdn.* │ │ everyonecook.* │ │ (Shield Std) │ │ Next.js 15 │ └──────┬──────────┘ └──────────────────┘ │ OAC ┌──────▼──────────────────────────────┐ │ S3 Buckets (4) │ │ - Content (avatars, posts) │ │ - Logs (CloudWatch archive) │ │ - Emails (SES incoming) │ │ - CDN Logs (CloudFront) │ │ Intelligent-Tiering + KMS │ └─────────────────────────────────────┘ ┌────────────────────────────────────────────────────────────────┐ │ API Gateway (api.*) │ │ Cognito Authorizer + WAF │ └──────────────────────────┬─────────────────────────────────────┘ │ ┌────────▼────────┐ │ API Router │ │ Lambda │ └────────┬────────┘ │ ┌──────────────────┼──────────────────┐ │ │ │ ┌────▼────┐ ┌─────▼─────┐ ┌─────▼─────┐ │ Auth │ │ Social │ │ Recipe │ │ Module │ │ Module │ │ + AI │ └────┬────┘ └─────┬─────┘ └─────┬─────┘ │ │ │ ┌────▼────┐ ┌─────▼─────┐ │ │ Admin │ │ Upload │ │ │ Module │ │ Module │ │ └────┬────┘ └─────┬─────┘ │ │ │ │ └──────────────────┼──────────────────┘ │ ┌──────────────────┼──────────────────┐ │ │ │ ┌────▼────────┐ ┌────▼────────┐ ┌────▼────────┐ │ DynamoDB │ │ OpenSearch │ │ Bedrock AI │ │ Single │ │ Vietnamese │ │ Claude 3.5 │ │ Table │ │ Analyzer │ │ Sonnet v2 │ └─────────────┘ └─────────────┘ └─────────────┘ ┌────────────────────────────────────────────────────────────────┐ │ SQS Queues (6) │ │ AI | Email | SearchIndex | Image | Analytics | Notification │ └──────────────────────────┬─────────────────────────────────────┘ │ ┌────────▼────────┐ │ Search Sync │ │ Worker Lambda │ └─────────────────┘ ┌────────────────────────────────────────────────────────────────┐ │ Cognito User Pool + 5 Lambda Triggers │ │ Post-Confirmation | Pre-Auth | Post-Auth | Custom-Msg | Pre-Signup │ └────────────────────────────┬───────────────────────────────────┘ │ ┌────────▼────────┐ │ SES Email │ │ DKIM + SPF │ └─────────────────┘ ┌────────────────────────────────────────────────────────────────┐ │ CloudWatch Dashboards + Alarms + X-Ray Tracing │ └────────────────────────────────────────────────────────────────┘ Workshop Flow This workshop follows a practical application development workflow:\nSetup Environment - Install tools (Node.js, AWS CLI, CDK CLI) CDK Bootstrap - Prepare AWS account for CDK deployments Configure Stacks - Set up infrastructure configuration (DNS, Certificate, Core, Auth, Backend, Observability) Deploy Infrastructure - Deploy all CDK stacks to AWS Configure API \u0026amp; Lambda - Set up API Gateway routes and Lambda functions Deploy Backend - Deploy API and Lambda code Test Endpoints - Verify all endpoints work end-to-end Push to GitLab - Version control and CI/CD setup Deploy to Amplify - Deploy frontend to AWS Amplify Monitor \u0026amp; Maintain - Use CloudWatch and X-Ray for monitoring What You\u0026rsquo;ll Learn Infrastructure as Code with AWS CDK (TypeScript) Serverless architecture with API Router pattern DynamoDB Single Table Design with username-based PK OpenSearch with Vietnamese language analyzer CloudFront CDN with Origin Access Control (OAC) Cognito authentication with Lambda triggers Lambda function modular organization (6 modules) SQS-based async processing with workers Bedrock AI integration with caching strategy WAF security for API Gateway CloudWatch monitoring and X-Ray tracing Cost optimization strategies ($15-24/day for 10K users) Key Features Vietnamese Support: OpenSearch Vietnamese analyzer for ingredient search AI-Powered: Bedrock Claude 3.5 Sonnet v2 for recipe generation Dictionary-First Translation: 99% coverage target with intelligent caching Social Platform: Posts, comments, reactions, friends, notifications Field-Level Privacy: Granular control over profile visibility Content Moderation: Automated and manual moderation workflows Advanced Search: Full-text search with Vietnamese normalization Cost Optimized: Intelligent-Tiering, caching, and resource optimization "},{"uri":"https://hviethub.github.io/Internship-Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS Event Information Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate: Saturday, November 15, 2025\nTime: 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nEvent Objectives Understand the AI/ML landscape in Vietnam Learn about AWS AI/ML services, particularly Amazon SageMaker Explore Generative AI capabilities with Amazon Bedrock Gain hands-on experience through live demonstrations Network with AWS professionals and fellow participants Event Schedule 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML Platform\nThe session covered comprehensive aspects of Amazon SageMaker:\nData Preparation and Labeling\nTools for data preprocessing Built-in labeling workflows Integration with data sources Model Training, Tuning, and Deployment\nDistributed training capabilities Hyperparameter optimization One-click deployment options A/B testing and model monitoring Integrated MLOps Capabilities\nModel registry and versioning CI/CD for ML workflows Automated model retraining Performance monitoring and drift detection Live Demo: SageMaker Studio Walkthrough\nHands-on demonstration of SageMaker Studio interface End-to-end ML workflow example Best practices for model development 10:30 – 10:45 AM | Coffee Break Networking opportunity with speakers and participants\n10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models Overview\nComprehensive comparison of available models:\nClaude (Anthropic)\nStrengths: Long context, reasoning, safety Use cases: Complex analysis, content generation Llama (Meta)\nStrengths: Open-source, customizable Use cases: Fine-tuning, specialized applications Titan (Amazon)\nStrengths: Cost-effective, AWS-native Use cases: Text generation, embeddings Prompt Engineering Techniques\nBasic Principles\nClear instructions and context Role definition and constraints Output format specification Advanced Techniques\nChain-of-Thought (CoT) reasoning Few-shot learning examples Zero-shot vs few-shot comparison Prompt templates and best practices Retrieval-Augmented Generation (RAG)\nArchitecture Overview\nVector databases and embeddings Semantic search integration Context retrieval strategies Knowledge Base Integration\nDocument ingestion and processing Chunking strategies Metadata management Query optimization Bedrock Agents\nMulti-step Workflows\nTask decomposition Sequential reasoning Error handling and retries Tool Integrations\nAPI connections Database queries External service calls Custom function execution Guardrails\nSafety Measures\nContent filtering Harmful content detection PII redaction Topic restrictions Content Filtering\nInput validation Output moderation Compliance enforcement Live Demo: Building a Generative AI Chatbot\nStep-by-step chatbot creation using Bedrock RAG implementation example Guardrails configuration Real-time Q\u0026amp;A with the chatbot 12:00 PM | Lunch Break (Self-arranged) Key Highlights Amazon SageMaker Insights End-to-End ML Platform\nSageMaker provides a complete solution from data preparation to model deployment Integrated MLOps capabilities reduce time-to-production Built-in algorithms and frameworks support various ML use cases Studio interface simplifies the ML workflow Key Features Demonstrated:\nAutomated data labeling reduces manual effort Distributed training accelerates model development One-click deployment simplifies production rollout Model monitoring ensures ongoing performance Generative AI with Bedrock Foundation Model Selection\nDifferent models excel at different tasks Consider factors: cost, performance, context length, safety Claude excels at reasoning and long-context tasks Llama offers flexibility through open-source customization Titan provides cost-effective AWS-native solutions Prompt Engineering Best Practices\nClear, specific instructions yield better results Chain-of-Thought improves reasoning quality Few-shot examples guide model behavior Iterative refinement is essential RAG Architecture Benefits\nGrounds AI responses in factual data Reduces hallucinations significantly Enables domain-specific knowledge integration Keeps information current without retraining Bedrock Agents Capabilities\nAutomate complex multi-step workflows Integrate with existing systems and APIs Handle dynamic decision-making Provide transparency in reasoning process Guardrails Importance\nEssential for production deployments Protect against harmful content Ensure compliance with regulations Maintain brand safety and reputation Key Takeaways Technical Knowledge Amazon SageMaker:\n✅ Comprehensive platform for entire ML lifecycle ✅ MLOps integration accelerates deployment ✅ Scalable infrastructure handles large workloads ✅ Cost optimization through managed services Amazon Bedrock:\n✅ Multiple foundation models for different needs ✅ Prompt engineering is critical for quality outputs ✅ RAG architecture enhances accuracy and relevance ✅ Agents enable complex workflow automation ✅ Guardrails are essential for safe production use Practical Applications Use Cases Identified:\nCustomer service chatbots with RAG Content generation and summarization Code generation and documentation Data analysis and insights extraction Automated workflow orchestration Implementation Considerations:\nStart with clear use case definition Choose appropriate foundation model Implement RAG for domain-specific knowledge Configure guardrails before production Monitor and iterate based on performance Best Practices ML Development:\nUse SageMaker Studio for unified development Implement MLOps from the start Monitor model performance continuously Plan for model retraining and updates GenAI Development:\nTest multiple foundation models Invest time in prompt engineering Implement RAG for factual accuracy Use agents for complex workflows Always configure guardrails Applying to Work Immediate Actions Explore SageMaker:\nSet up SageMaker Studio environment Experiment with built-in algorithms Practice data preparation workflows Test model deployment options Experiment with Bedrock:\nTry different foundation models Practice prompt engineering techniques Build a simple RAG prototype Test guardrails configuration Short-term Goals ML Projects:\nIdentify suitable ML use cases in current projects Propose SageMaker for next ML initiative Implement MLOps best practices Set up model monitoring GenAI Projects:\nBuild proof-of-concept chatbot Implement RAG for knowledge base Create Bedrock agent for workflow automation Establish guardrails standards Long-term Vision Organizational Impact:\nEvangelize AI/ML adoption Establish best practices and standards Build reusable components and templates Create knowledge sharing culture Skill Development:\nPursue AWS ML certifications Deep dive into specific models Master prompt engineering Learn advanced RAG techniques Event Experience Learning Environment Professional Setting:\nAWS Vietnam Office provided excellent facilities Well-organized schedule with appropriate breaks Interactive sessions encouraged participation Live demos enhanced understanding Expert Speakers:\nAWS specialists shared real-world insights Practical examples from production systems Clear explanations of complex concepts Responsive to questions and discussions Hands-on Learning Live Demonstrations:\nSageMaker Studio walkthrough was comprehensive Bedrock chatbot demo showed practical implementation Real-time problem-solving demonstrated best practices Interactive elements kept engagement high Practical Insights:\nLearned from actual production use cases Understood common pitfalls and solutions Gained confidence to start own projects Received guidance on next steps Networking Opportunities Peer Connections:\nMet fellow AI/ML enthusiasts Exchanged ideas and experiences Discussed potential collaborations Built professional network AWS Community:\nConnected with AWS specialists Learned about AWS resources and support Discovered community events and programs Identified mentorship opportunities Personal Growth Technical Skills:\nExpanded knowledge of AWS AI/ML services Gained practical GenAI experience Understood MLOps principles Learned industry best practices Professional Development:\nIncreased confidence in AI/ML domain Identified career development paths Recognized areas for further learning Motivated to pursue certifications Reflections What Worked Well Content Quality:\nWell-structured agenda covered essential topics Balance between theory and practice Appropriate depth for target audience Relevant and current information Delivery:\nEngaging presentation style Clear explanations with examples Interactive demonstrations Good time management Logistics:\nProfessional venue and setup Smooth registration process Adequate breaks and networking time Excellent organization overall Areas for Improvement Suggestions:\nMore hands-on lab time would be beneficial Provide pre-event materials for preparation Include more advanced topics for experienced participants Offer follow-up sessions or office hours Overall Assessment The AWS Cloud Mastery Series #1 event was an excellent introduction to AI/ML and GenAI on AWS. The combination of comprehensive content, expert speakers, live demonstrations, and networking opportunities created a valuable learning experience.\nKey Benefits:\n✅ Solid foundation in AWS AI/ML services ✅ Practical knowledge of Bedrock and SageMaker ✅ Confidence to start own AI/ML projects ✅ Professional network expansion ✅ Clear path for continued learning Impact: This event has significantly enhanced my understanding of AI/ML on AWS and provided practical knowledge that I can immediately apply to work projects. The exposure to both SageMaker and Bedrock has opened new possibilities for implementing AI solutions.\nEvent Photos Conclusion Attending the AWS Cloud Mastery Series #1 was a valuable investment in professional development. The event provided not only technical knowledge but also practical insights, networking opportunities, and motivation to pursue AI/ML initiatives.\nNext Steps:\nComplete hands-on labs with SageMaker and Bedrock Build proof-of-concept projects Share learnings with team Attend future AWS Cloud Mastery Series events Pursue AWS ML certification The event successfully achieved its objectives of introducing AWS AI/ML services and inspiring participants to explore and implement AI solutions in their work.\n"},{"uri":"https://hviethub.github.io/Internship-Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Apex Legends Migrates to Amazon GameLift Servers in Just 10 Days By Michael Jackson, David Holladay, and Juho Jantunen on 05/12/2025 in Amazon GameLift, Game Development, Games, Industries\nRespawn Entertainment recently migrated Apex Legends to Amazon GameLift servers, completing the traffic cutover in just 10 days with no downtime to players.\nA core priority for Respawn was to perform technical integration and validation without disrupting development teams or the Apex player base. AWS met that need by partnering with AWS Partner Code Wizards Group and leveraging the Amazon GameLift Migration Acceleration Program to deliver a staged, low-risk, comprehensive migration plan — resulting in one of the largest AWS-led infrastructure transitions ever undertaken for a live service game.\nSetting Priorities As a live service game, Apex Legends continually evolves — not only in gameplay and content but also in operational quality. With millions of players worldwide, Respawn needed more observability, consistency, and flexibility in their infrastructure. When exploring modern hosting options with AWS and Amazon GameLift, they set three clear goals:\nExpand Player Experience\nInfrastructure needed to scale to meet player demand spikes during new seasons and events.\nModernize Hosting\nMoving from hybrid infrastructure with bare metal to cloud-based would deliver scalable gameplay environments for the global player community.\nMaintain Development Velocity\nThe new solution had to integrate smoothly with Respawn\u0026rsquo;s existing development, testing, and deployment workflows — without slowing progress.\n\u0026ldquo;Minimizing the disruption for our player population was a critical success factor for the migration. The coordinated effort between Respawn and the Amazon GameLift Servers team made this a reality.\u0026rdquo;\n— Robert LaCruise, Principal Engineer, Online Game Services, Apex Legends\nPlanning the Migration To ensure Amazon GameLift Servers could meet these requirements, Respawn partnered with AWS and Code Wizards Group — a long-standing migration partner in the gaming industry — to conduct a detailed assessment.\nAmazon GameLift brings experience from powering popular games like Dead by Daylight, Mortal Kombat 1, and Marvel SNAP, with the ability to scale to 100 million concurrent players.\nThe teams identified 3 key technical proof-of-concepts:\nMain Flow: Smooth matchmaking and server placement for all players.\nZero Downtime Updates: Ability to deploy updates without disruption.\nMetrics \u0026amp; Observability: Real-time data to monitor and improve player experience.\n\u0026ldquo;We believe their (AWS) services offer the best overall player facing performance when compared to other alternatives. Additionally, AWS servers are standard for many other FPS titles and we believe our players will see the benefits of this migration globally.\u0026rdquo;\n— Robert LaCruise, Principal Engineer, Online Game Services, Apex Legends\nProof-of-Concept Testing To validate the migration strategy, the teams built and tested multiple new technologies.\nSeamless Matchmaking The first step was building a \u0026ldquo;player redirect layer\u0026rdquo; that could route players to Amazon GameLift Servers without changing the game\u0026rsquo;s backend logic, avoiding risky changes and unnecessary QA cycles.\nAdditionally, a Game Server Wrapper was created to leverage the existing game server build without modification while adding observability and logging. Both were developed by Code Wizards Group and were key to the migration.\n\u0026ldquo;We knew that player experience was super important to Respawn and their players. We created the Wrapper and Portal to enable this. The end result was no downtime to the player experience.\u0026rdquo;\n— Stuart Muckley, CEO \u0026amp; Founder, Code Wizards Group\nZero Downtime Updates Code Wizards developed a system to automatically move players to upgraded servers after matches end — allowing updates to be deployed without disrupting gameplay.\n\u0026ldquo;Player-facing goals were formalized into our architecture designs… ensuring that zero downtime deployments were a reality from start to finish.\u0026rdquo;\n— Jared Cugno, Head of Engineering, Apex Legends\nReal-Time Monitoring Monitoring and observability services gave Respawn clear visibility into session health, latency, and server performance — enabling timely detection and resolution of issues.\n\u0026ldquo;We are leveraging the global infrastructure of AWS to continuously optimize server and network performance. We have more options and flexibility on the new infrastructure platform.\u0026rdquo;\n— Robert LaCruise, Principal Engineer, Online Game Services, Apex Legends\nBackend Benefits After testing and optimization, Respawn now benefits from:\nAWS\u0026rsquo;s global, low-latency network AWS tools for debugging and server tuning Cost optimization with multiple compute instance choices Preserved existing development pipeline through wrapper and redirect layer More control via Amazon GameLift Servers APIs like TerminationGameSession and queue placement override This means Respawn didn\u0026rsquo;t need to slow development or change pipelines but still got better tools — without sacrificing speed.\nFigure 1: The \u0026ldquo;Portal\u0026rdquo; translation layer redirects traffic from existing backend to Amazon GameLift Servers.\nThe 10-Day Migration After the assessment phase, Respawn proceeded with region-by-region traffic cutover, starting with the smallest regions to minimize impact and build confidence gradually.\nAWS provided:\nDirect technical support Real-time monitoring tools Pre-tested rollback systems Before cutover, teams:\nDefined rollback thresholds Validated alerting pipelines Ran simulated migration tests During migration, they continuously monitored:\nPlayer connections Latency and jitter Queue wait times Match placement success rates Thanks to automation and real-time alerts, teams were ready to rollback if needed. But in reality, the process went smoothly with no downtime throughout the 10 days.\nRespawn shared initial results in last month\u0026rsquo;s update with the community and continues to monitor and optimize player experience.\nConclusion The migration of Apex Legends to Amazon GameLift Servers demonstrates what\u0026rsquo;s achievable when infrastructure modernization is player-centered.\nBacked by AWS and Code Wizards Group, Respawn executed a large-scale migration in record time without disrupting gameplay or development pipelines.\n\u0026ldquo;The migration team were put through their paces several times… They did it well, and I\u0026rsquo;m looking forward to seeing what the future holds.\u0026rdquo;\n— Jared Cugno, Head of Engineering, Apex Legends\nAuthors Michael Jackson:\nMichael Jackson is the Global Business Development Director for Amazon GameLift Services, focused on game server and game streaming solutions at Amazon. He has experience in managed hosting and cloud technology designed specifically for the gaming industry. Michael has helped some of the world\u0026rsquo;s most creative studios modernize infrastructure, reduce operational risk, and deploy at global scale.\nDavid Holladay:\nDavid Holladay is the Head of Game Product Marketing at AWS.\nJuho Jantunen:\nJuho Jantunen is a Global Principal Solutions Architect on the AWS for Games team, specializing in game server hosting and backend solutions. He has experience in the gaming industry and cloud technology, and has built and operated game backends on AWS for multiple titles with millions of players.\n"},{"uri":"https://hviethub.github.io/Internship-Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"DevOps on AWS Event Information Date: Monday, November 17, 2025\nTime: 8:30 AM – 12:00 PM (Morning Session)\nLocation: Bitexco Financial Tower, HCMC\nSpeakers: Truong Quan Tinh, Kha Van, Bao,Thinh\nRole: Attendee\nEvent Overview Full-day workshop covering DevOps culture, AWS CI/CD services, and Infrastructure as Code practices.\nKey Topics DevOps Mindset (8:30 – 9:00 AM)\nDevOps culture and principles DORA metrics: Deployment Frequency, Lead Time, MTTR, Change Failure Rate Benefits: Faster delivery, lower failure rates, quicker recovery AWS CI/CD Pipeline (9:00 – 10:30 AM)\nCodeCommit: Git-based source control CodeBuild: Automated build and test CodeDeploy: Deployment strategies Blue/Green: Zero downtime, instant rollback Canary: Gradual rollout with monitoring Rolling: Cost-effective batch updates CodePipeline: End-to-end orchestration Live demo: Complete CI/CD pipeline Infrastructure as Code (10:45 AM – 12:00 PM)\nCloudFormation: AWS-native IaC with templates Declarative YAML/JSON Stack management and drift detection AWS CDK: Programming language approach TypeScript, Python, Java, C#, Go Higher-level constructs and reusable patterns Live demo: Deploying with both tools Key Takeaways DevOps Culture:\nCultural transformation, not just tools DORA metrics provide objective measurement Elite performers deploy 208x more frequently CI/CD Best Practices:\nAutomate source, build, test, deploy stages Choose deployment strategy based on use case Implement proper testing at each stage Use manual approvals for production IaC Implementation:\nCloudFormation for AWS-native declarative IaC CDK for programming language approach with type safety Version control all infrastructure code Implement drift detection Next Steps:\nMeasure current DORA metrics Implement CI/CD pipeline for key application Convert infrastructure to IaC (CloudFormation or CDK) Pursue AWS DevOps certification Event Photos Rating: ⭐⭐⭐⭐⭐ (5/5)\nComprehensive DevOps workshop with practical CI/CD demonstrations and clear IaC implementation strategies.\n"},{"uri":"https://hviethub.github.io/Internship-Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: PHAM MINH HOANG VIET\nPhone Number: 0976236132\nEmail: vietpmhse181851@fpt.edu.vn\nUniversity: FPT University\nMajor: IA\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations - Learn markdown language 09/08/2025 09/08/2025 https://github.com/lucthienphong1120/Markdown-syntax?tab=readme-ov-file#6-escape-markdown 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database - Translate Blog 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ https://aws.amazon.com/blogs/machine-learning/enabling-customers-to-deliver-production-ready-ai-agents-at-scale/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI + How to use Account Authentication Support - Learn about create Budgets - Practice: + How to create Budget by template + How to create Cost Budget + How to create Usage Budget + How to create a Reservation Insance(RI) + How to create a Savings Plans Budget + Clear Up Budget - Learn about AWS support Packages - Practice: + Types of support requests + Change support package + Manage support requests 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ https://000001.awsstudygroup.com/vi/ https://000007.awsstudygroup.com/vi/ https://000009.awsstudygroup.com/vi/ 5 - Learn AWS Virtual Private Cloud - PC Security and Multi-VPC features - VPN - DirectConnect - LoadBalancer - ExtraResources Practice: + Start with Amazon VPC and AWS VPN Site-to-Site ( Introduction) + Subnets + Route table + Internet Gateway (IGW) + NAT Gateway + Security Group + Network ACLs + VPC Resource Map 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ https://000003.awsstudygroup.com/ 6 - Practice: + Create VPC + Create Subnet + Creating an Internet Gateway + Creating Route Tables + Creating Security Groups + Enabling VPC Flow Logs 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ https://000003.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\nCreate different types of Budgets according to usage needs\nKnow how to use AWS support packages\nCreate VPC, subnet, Internet gateway, routing table, security group and enable VPC flow log\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand Amazon VPC Can deploy a comprehensive EC2 infrastructure following AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deploying Amazon EC2 Instances - Practice: + Create EC2 Server + Test Connection + Create NAT Gateway + Using Reachability Analyzer + Create EC2 Instance Connect Endpoint + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting 09/15/2025 09/15/2025 https://000003.awsstudygroup.com/ 3 - Setting Up Site-to-Site VPN Connection in AWS - Practice: + Create a VPN environment + Configure VPN Connection + VPN Connection using Strongswan with Transit Gateway + Clean up resources 09/16/2025 09/16/2025 https://000003.awsstudygroup.com/ 4 - Set up Hybrid DNS with Route 53 Resolver - Practice: + Generate Key Pair + Initialize CloudFormation Template + Configure Security Group + Connecting to RDGW + Microsoft AD Deployment - Setup DNS - Practice: + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results + Clean up resources 09/17/2025 09/17/2025 https://000010.awsstudygroup.com/ 5 - Translate Blog - Set up VPC peering - Practice: + Initialize CloudFormatuon Templates + Create Security Group + Create EC2 Instance + Update Network ACL(NACLs) + Create VPC Peering + Configure Route Tables + Enable Cross-Peer DNS + Clean up resources 09/18/2025 09/18/2025 https://000019.awsstudygroup.com/ 6 - Set up AWS Transit Gateway - Practice: + Generate Key Pair + Initialize CloudFormation Template + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables + Clean up resoucre 09/19/2025 09/19/2025 https://000020.awsstudygroup.com/ Week 2 Achievements: Deploying Amazon EC2 instance success Set up successfull VPC peering,DNS,Hybrid DNS with Route 53 resolver and AWS Transit Gateway, Site-to-Site VPN Connection "},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand Amazon Elastic Compute Cloud(EC2):Instance type,AMI/Backup/Key pair,Elastic block store,Instance store,User data,Meta data STARTING WITH AMAZON S3 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Compute VM on AWS - Amazon Elastic Compute Cloud(EC2): + Instance type + AMI/Backup/Key pair + Elastic block store + Instance store + User data + Meta data - EC2 Autoscaling - EFS/FSX - Lightsail - MGN - Deploy AWS Backup to the System - Practice: + Create S3 Bucket + Deploy infrastructure + Create Backup plan + Set up notifications + Test Restore + Clean up resources 09/22/2025 09/22/2025 https://000013.awsstudygroup.com/ 3 - Using File Storage Gateway - Practice: + Create S3 Bucket + Create EC2 for Storage Gateway - Using AWS Storage Gateway - Practice: + Create Storage Gateway + Create File Shares + Mount File shares on On-premises machine - Clean up resources 09/23/2025 09/23/2025 https://000024.awsstudygroup.com/ 4 - Startting with Amazon S3 - Practice: + Create S3 Bucket + Download data + Enable static website hosting + Configuring public access block + Configure public object access + Check the website - Accelerate Static Websites with Cloudfront - Practice: + Block all public access + Config Amazon CloudFront + Test Amazon Cloudfront + Bucket Versioning + Move objects + Replication Object multi Region + Clean up resources 09/24/2025 09/24/2025 https://000057.awsstudygroup.com/ 5 - AWS Storage Services - Amazon Simple Storage Service(S3)/Access Point/Storage Class - S3 Static Website \u0026amp; CORS/Control Access/Object Key \u0026amp; Performance/Glacier - Snow Family - Storage Gateway/Backup - Deploy AWS Backup to the System (Second time - Module 04) - Practice: + Create S3 Bucket + Deploy infrastructure + Create Backup plan + Set up notifications + Test Restore + Clean up resources 09/25/2025 09/25/2025 https://000013.awsstudygroup.com/ 6 -Translate Blog - VMWare Workstation - Practice: + Export Virtual Machine from On-premises + Upload virtual machine to AWS + Import virtual machine to AWS + Deploy Instance from AMI + Setting up S3 bucket ACL + Export virtual machine from Instance + Export virtual machine from AMI + Clean up resources 09/26/2025 09/26/2025 https://000014.awsstudygroup.com/ Week 3 Achievements: Able to deploy Amazon Elastic Compute Cloud (EC2): Instance type, AMI/Backup/Key pair, Elastic block store, Instance store, User data, Metadata. Know how to use Amazon S3. "},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Using File Storage Gateway -Practice: + Create S3 bucket + Create EC2 for Storage Gateway + Create Storage Gateway + Create File Shares + Mounting File Shares on On-premise Machine - Clean up resources 09/29/2025 09/29/2025 https://000024.awsstudygroup.com/ 3 - Amazon FSx for Windows File Server - Practice: + Create Environment + Create an SSD Multi-AZ file system + Create an HDD Multi-AZ file system + Create new file shares + Test Performance + Monitor Performance + Enable data deduplication + Enable shadow copies + Manage user sessions and open files + Enable user storage quotas + Scale throughput capacity + Scale storage capacity + Delete environment + Using the AWS CLI (reference) 09/30/2025 09/30/2025 https://000025.awsstudygroup.com/ 4 - Startting with Amazon S3 - Practice: + Create S3 Bucket + Download data + Enable static website hosting + Configuring public access block + Configure public object access + Check the website - Accelerate Static Websites with Cloudfront - Practice: + Block all public access + Config Amazon CloudFront + Test Amazon Cloudfront + Bucket Versioning + Move objects + Replication Object multi Region + Clean up resources 10/1/2025 10/1/2025 https://000057.awsstudygroup.com/ 5 - Share Responsibility Model - Amazon Identity and access management - Amazon Cognito - AWS Organization - AWS Identity Center - Amazon Key Management Service - AWS Security Hub - Hands-on and Additional research 10/2/2025 10/2/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Enable Security Hub + Score for each set of criteria + Clean up resources 10/3/2025 10/3/2025 https://000018.awsstudygroup.com/ Week 4 Achievements: Practiced deploying AWS Storage Gateway and connecting on-prem systems to S3 via file shares.\nBuilt Amazon FSx for Windows file systems, tested performance, enabled data services (dedup, shadow copies), and managed scaling.\nConfigured Amazon S3 for static website hosting, CloudFront acceleration, versioning, and cross-Region replication.\nReviewed and practiced AWS security fundamentals: IAM, Cognito, Organizations, Identity Center, KMS.\nEnabled and analyzed findings in AWS Security Hub to assess account security posture.\nStrengthened overall skills in cloud storage, file sharing, web hosting, identity management, and security monitoring.\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Optimizing EC2 Costs with Lambda - Practice: + Create VPC + Create Security Group + Create EC2 instance + Incoming Web-hooks slack + Create Tag for Instance + Create Role for Lambda Function + Create Lambda Function + Function stop instance + Function start instance + Check the result + Clean up resources 10/06/2025 10/06/2025 https://000022.awsstudygroup.com/ 3 - Manage Resources Using Tags and Resource Groups - Practice: + Create EC2 Instance with Tags + Managing Tags in AWS Resources + Filter resources by tag + Using tags with CLI + Create a Resource Group + Clean up resources 10/07/2025 10/07/2025 https://000027.awsstudygroup.com/ 4 - Manage access to EC2 services with resoucres tags through IAM services - Practice: + Create IAM user + Create IAM Policy + Create IAM Role + Switch Role + Initiating access to EC2 console in AWS Region - Tokyo + Initiating access to EC2 console in AWS Region - North Virginia + Edit Resource Tag on EC2 Instance + Policy Check + Clean up resources 10/08/2025 10/08/2025 https://000028.awsstudygroup.com/ 5 - Limitation of user rights with IAM permission boundary - Practice: + Create Restriction Policy + Create IAM Limited User + Test IAM User Limit + Clean up resources 10/09/2025 10/09/2025 https://000030.awsstudygroup.com/ 6 - Encrypt at rest with AWS KMS - Practice: + Create Policy and Role + Create Group and User + Create Key Management Service + Create Bucket + Upload data to S3 + Create CloudTrail + Logging to CloudTrail + Create Amazon Athena + Retrieve data with Athena + Test and share encrypted data on S3 + Clean up resources 10/10/2025 10/10/2025 https://000033.awsstudygroup.com/ Week 5 Achievements: Automated EC2 cost optimization using Lambda functions (start/stop automation, tagging strategy, Slack notifications).\nImproved resource organization by effectively using tags and creating Resource Groups for better visibility and management.\nEnhanced access control skills by applying IAM policies tied to resource tags and validating permissions across multiple AWS Regions.\nStrengthened IAM governance through implementing permission boundaries to enforce strict user limitations.\nGained strong hands-on experience with AWS KMS encryption, S3 secure storage, CloudTrail auditing, and Athena log analysis.\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - IAM Role \u0026amp; Condition - Practice: + Create an IAM Group + Create IAM Users + Check User Permissions + Create Admin IAM Role + Configure Switch role + Restrict role access + Limit Switch role by IP + Limit switch role by Time + Clean up resources 10/13/2025 10/13/2025 https://000044.awsstudygroup.com/ 3 - Granting authorization for an application to access AWS services with an IAM role. - Practice: + Create EC2 Instance + Create S3 bucket + Generate IAM user and access key + Using access key + Generate IAM user and access key + Using IAM role + Clean up resources 10/14/2025 10/14/2025 https://000048.awsstudygroup.com/ 4 - Database Concepts review - Amazon RDS \u0026amp; Amazon Aurora - Redshift - Elasticache 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 - Amazon Relational Database Service (Amazon RDS) - Practice: + Create a VPC + Create EC2 Security Group + Create RDS Security Group + Create DB Subnet Group + Create EC2 instance + Create RDS database instance + Application Deployment + Backup and Restore + Clean up resources 10/16/2025 10/16/2025 https://000005.awsstudygroup.com/ 6 - Database Schema Conversion \u0026amp; Migration - Practice: + EC2 Connect RDP Client + EC2 Connect Fleet Manager + SQLSrv Src Config + Oracle connect SrcDB + Oracle config SrcDB + Drop Constraint + MSSQL to aur MySQL target config + MSSQL to aur MySQL create project + MSSQL to aur MySQL SchemConv + Oracle2 MySQL Schema conversion 1 + Create Mig Task And Endpoint + Inspect S3 + Create Severless Migration + Create Event Not + Logs + Troubleshoot Test Scenario Mem Pressure + TroubleShoot Test Scenario Table Err + Clean up resources 10/17/2025 10/17/2025 https://www.youtube.com/watch?v=cxwAOP1379s\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=229 Week 6 Achievements: Gained advanced skills in IAM by implementing conditional role access (IP, time), creating users, groups, and admin roles, and validating secure role switching.\nLearned how to securely grant application access to AWS services by replacing access keys with IAM roles on EC2.\nStrengthened understanding of AWS database services including RDS, Aurora, Redshift, and ElastiCache.\nSuccessfully deployed a full RDS environment (VPC, security groups, EC2, DB instance) and performed application deployment plus backup/restore operations.\nCompleted end-to-end database schema conversion and migration workflows for MSSQL→Aurora MySQL and Oracle→MySQL using SCT and DMS, including troubleshooting and event monitoring.\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Review for midterm exams Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review AWS services and features: + IAM (Identity and Access Management) + MFA (Multi-Factor Authentication) + SCP (Service Control Policy) 10/20/2025 10/20/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 3 - Review AWS services and features: + KMS (Key Management Service) + TLS / ACM (AWS Certificate Manager) + Security Groups + NACLs (Network Access Control Lists) 10/21/2025 10/21/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Review AWS services and features: + GuardDuty + Shield + WAF + Secrets Manager 10/22/2025 10/22/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026gt; 5 - Review AWS services and features: + Multi-AZ (Availability Zone) + Multi-Region + DR Strategies (Disaster Recovery Strategies) 10/23/2025 10/23/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 6 - Review AWS services and features: + Auto Scaling + Route 53 + Load Balancing (Elastic Load Balancer - ELB) + Backup \u0026amp; Restore 10/24/2025 10/24/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i Week 7 Achievements: *Review AWS services and features:\n*Security architecture design:\nIAM (Identity and Access Management) MFA (Multi-Factor Authentication) SCP (Service Control Policy) KMS(Key Management Service) TLS / ACM (AWS Certificate Manager) Security Groups NACLs (Network Access Control Lists) GuardDuty Shield WAF (Web Application Firewall) Secrets Manager *Flexible and sustainable architectural design:\nMulti-AZ Multi-Region DR Strategies Auto Scaling Route 53 Load Balancing Backup \u0026amp; Restore "},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review for midterm exams Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Compute scaling + EC2 Auto Scaling + Lambda + Fargate - Storage + S3 + EFS + EBS 10/27/2025 10/27/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 3 - Caching - Network Optimization + CloudFront + Global Accelerator 10/28/2025 10/28/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Cost Explorer - Budgets - Savings Plans 10/29/2025 10/29/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 5 - Lifecycle Policies - NAT Gateway Optimization - Storage Tiering 10/30/2025 10/30/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 6 - Midterm Exam 10/31/2025 10/31/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i Week 8 Achievements: "},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: *Continue studying and practicing lab of module 7\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Data Lake on AWS - Practice: + Creating an IAM Role + Create Policy + Create S3 Bucket + Creating a Delivery Stream + Create Sample Data + Create Glue Crawler + Data Check + Create SageMaker Notebook + Analysis with Athena + Visualize with QuickSight + Clean up resources 11/03/2025 11/03/2025 https://000035.awsstudygroup.com/ 3 - LHOL: Hands-on Labs for Amazon DynamoDB - Practice: + Getting Started + Explore DynamoDB with the CLI + Explore the DynamoDB Console + Backups + LMIG: Relational Modeling \u0026amp; Migration - LBED: Generative AI with DynamoDB zero-ETL to OpenSearch integration and Amazon Bedrock - Practice: + Getting Started + Service Configuration + Integrations + Query and Conclusion 11/04/2025 11/04/2025 https://000039.awsstudygroup.com/ 4 - LADV: Advanced Design Patterns for Amazon DynamoDB - Practice: + Getting Started + Exercise 1: DynamoDB Capacity Units and Partitioning + Exercise 2: Sequential and Parallel Table Scans + Exercise 3: Global Secondary Index Write Sharding + Exercise 4: Global Secondary Index Key Overloading 11/05/2025 11/05/2025 https://000039.awsstudygroup.com/ 5 - LADV: Advanced Design Patterns for Amazon DynamoDB - Practice: + Exercise 5: Sparse Global Secondary Indexes + Exercise 6: Composite Keys + Exercise 7: Adjacency Lists + Exercise 8: Amazon DynamoDB Streams and AWS Lambda 11/06/2025 11/06/2025 https://000039.awsstudygroup.com/ 6 - LCDC: Change Data Capture for Amazon DynamoDB - Practice: + Getting Started + Scenario Overview + Change Data Capture using DynamoDB Streams Change Data + Capture using Kinesis Data Streams + Summary and Clean Up 11/07/2025 11/07/2025 https://000039.awsstudygroup.com/ Week 9 Achievements: Built a complete Data Lake on AWS, including S3 storage, Glue cataloging, Kinesis delivery streams, Athena queries, SageMaker notebooks, and QuickSight visualizations.\nStrengthened NoSQL skills through hands-on DynamoDB labs, exploring CLI operations, backups, relational modeling patterns, and zero-ETL integrations with OpenSearch and Amazon Bedrock.\nLearned and applied advanced DynamoDB design patterns, including capacity planning, partitioning, parallel scans, write sharding, sparse indexes, composite keys, adjacency lists, and stream processing.\nImplemented Change Data Capture (CDC) workflows for DynamoDB using both DynamoDB Streams and Kinesis Data Streams.\nGained strong practical experience in data engineering, NoSQL data modeling, real-time data processing, and analytics on AWS.\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Getting Started with VPC, EC2, Site-to-Site VPN, and Transit Gateway\nWeek 3: Getting started with Amazon S3, EC2 Advanced Features, and AWS Backup\nWeek 4: Working with Storage Gateway, Amazon FSx, and AWS Security Services\nWeek 5: Optimizing Costs with Lambda, Resource Tagging, and KMS Encryption\nWeek 6: Advanced IAM, RDS Deployment, and Database Migration\nWeek 7: Reviewing Security and High Availability for Midterm Exam\nWeek 8: Reviewing Performance Optimization and Cost Management - Midterm Exam\nWeek 9: Building Data Lakes and Advanced DynamoDB Design Patterns\nWeek 10: Global Serverless Applications and Event-Driven Architecture with DynamoDB\nWeek 11: Analytics on AWS - Glue, Athena, Kinesis, Redshift, and QuickSight\nWeek 12: Finding and Fixing Bugs in the Project\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.2-setup-environment/","title":"Setup Environment","tags":[],"description":"","content":"Overview Trong bước này, bạn sẽ cài đặt tất cả các công cụ cần thiết để phát triển và deploy ứng dụng EveryoneCook.\nRequired Tools 1. Node.js 20.x\n# Download from https://nodejs.org/ # Or use nvm (recommended) nvm install 20 nvm use 20 # Verify installation node --version # Should be v20.x npm --version Screenshot: Terminal showing Node.js 20.x installed\n2. AWS CLI v2\n# Windows: Download from https://aws.amazon.com/cli/ # macOS: brew install awscli # Linux: curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install # Verify installation aws --version 3. AWS CDK CLI\n# Install CDK globally npm install -g aws-cdk # Verify installation cdk --version Screenshot: Terminal showing CDK CLI installed\n4. Git\n# Download from https://git-scm.com/ # Or use package manager # Verify installation git --version 5. Code Editor\nRecommended: Visual Studio Code with extensions:\nAWS Toolkit GitLab Workflow ESLint Prettier AWS Account Setup 1. Create AWS Account\nIf you don\u0026rsquo;t have an AWS account:\nGo to https://aws.amazon.com/ Click \u0026ldquo;Create an AWS Account\u0026rdquo; Follow the registration process Add payment method 2. Create IAM User\nFor security, don\u0026rsquo;t use root account:\nGo to IAM Console → Users → Create user User name: everyonecook-dev Enable \u0026ldquo;Provide user access to the AWS Management Console\u0026rdquo; Attach policies: AdministratorAccess (for workshop, use more restrictive in production) Create user and save credentials Screenshot: IAM console showing user created with AdministratorAccess\n3. Configure AWS CLI\n# Configure AWS credentials aws configure # Enter: # AWS Access Key ID: [Your Access Key] # AWS Secret Access Key: [Your Secret Key] # Default region name: us-east-1 # Default output format: json Screenshot: Terminal showing aws configure completed\n4. Verify AWS Access\n# Test AWS credentials aws sts get-caller-identity # Should return your account ID and user ARN Domain Setup (Optional) If you want to use a custom domain:\n1. Register Domain\nOptions:\nRoute 53 (AWS) Hostinger GoDaddy Namecheap For this workshop, we use: everyonecook.cloud\n2. Note Domain Registrar\nYou\u0026rsquo;ll need access to domain registrar to update nameservers later.\nGitLab Setup 1. Create GitLab Account\nGo to https://gitlab.com/ Sign up for free account Verify email 2. Create Personal Access Token\nGo to GitLab → Settings → Access Tokens Token name: everyonecook-deploy Scopes: api, read_repository, write_repository Create token and save it securely Screenshot: GitLab showing personal access token created\n3. Configure Git\n# Set your name and email git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; # Verify configuration git config --list Project Setup 1. Clone or Create Project\nOption A: Clone existing project\ngit clone https://gitlab.com/your-username/everyonecook.git cd everyonecook Option B: Create new project\nmkdir everyonecook cd everyonecook git init 2. Install Dependencies\n# Install all dependencies npm install # This installs: # - Infrastructure dependencies (CDK) # - Backend dependencies (Lambda modules) # - Shared dependencies 3. Copy Environment Variables\n# Copy example env file cp .env.example .env # Edit .env with your values # Key variables: # - AWS_REGION=us-east-1 # - AWS_ACCOUNT_ID=your-account-id # - DOMAIN_NAME=everyonecook.cloud # - GITLAB_TOKEN=your-gitlab-token Verification Check that everything is installed correctly:\n# Check Node.js node --version # v20.x # Check npm npm --version # 10.x # Check AWS CLI aws --version # aws-cli/2.x # Check CDK cdk --version # 2.x # Check Git git --version # 2.x # Check AWS credentials aws sts get-caller-identity # Check project dependencies npm list --depth=0 Screenshot: Terminal showing all tools verified\nTroubleshooting Issue: Node.js version mismatch\n# Use nvm to switch versions nvm install 20 nvm use 20 Issue: AWS CLI not found\nRestart terminal after installation Check PATH environment variable Issue: CDK command not found\n# Reinstall CDK globally npm uninstall -g aws-cdk npm install -g aws-cdk Issue: AWS credentials invalid\n# Reconfigure AWS CLI aws configure # Enter correct credentials Cost Estimate Development Environment:\nAWS Free Tier covers most services Estimated cost: $20-55/month Main costs: DynamoDB, S3, CloudFront, Lambda Tips to minimize costs:\nUse pay-per-request for DynamoDB Enable S3 Intelligent-Tiering Disable OpenSearch in dev (enable only when needed) Delete resources when not in use Next Steps Once your environment is set up, proceed to CDK Bootstrap to prepare your AWS account for CDK deployments.\n"},{"uri":"https://hviethub.github.io/Internship-Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"6 Lessons Game Developers Can Learn from Epic Games\u0026rsquo; Cloud Governance Strategy By Gena Gizzi and Ryan Rumble, on 03/12/2025 in Amazon EC2, Amazon EBS, Amazon ElastiCache, Amazon EventBridge, Amazon RDS, Application Integration, Application Services, AWS Lambda, AWS Step Functions, Compute, Customer Solutions, Database, Game Development, Games, Graviton, Industries, Storage\nThis post is co-authored by Reza Nikoopour, Principal Engineer, Cloud Governance at Epic Games.\nFor game studios large and small, a smart cloud governance strategy can play a crucial role in reducing costs and enhancing security for a live service game. Here are some key lessons from Epic Games\u0026rsquo; experience working with Amazon Web Services (AWS) that game developers should consider when building their cloud governance strategy.\nSince Fortnite launched in 2017, Epic\u0026rsquo;s player base has reached 500 million registered accounts. Fortnite is built on AWS, using services like Amazon EC2 for backend infrastructure. To this day, Epic continues to rely on AWS technology to operate its global server fleet.\nWith AWS, Epic can automatically scale compute capacity based on player demand fluctuations as the game continues to grow.\n\u0026ldquo;AWS has been instrumental in guiding us as we established our cloud governance team at Epic. As a result, we\u0026rsquo;ve reduced cloud costs by approximately 20% while improving resource observability across the entire system.\u0026rdquo;\n— Shane Smith, Vice President of Technology Services at Epic Games\nThis was achieved in part through an initiative launched in 2022 aimed at achieving cloud efficiency, not just proficiency, while transforming Epic\u0026rsquo;s backend to support Fortnite\u0026rsquo;s continued growth.\n1. Establish Guardrails Game developers often dream of releasing a blockbuster, but predicting popularity is very difficult. Strategies like tagging, access management, and cloud resource governance are often not prioritized during rapid growth phases. When a game unexpectedly becomes a phenomenon, the focus typically shifts entirely to maintaining player experience.\nHowever, cloud governance is vital because it provides a set of rules, processes, and reports that guide best practices. Ensuring comprehensive visibility of resources and understanding critical game pathways will help developers establish a stronger position in the long term.\n2. Leverage AWS Expertise To improve cloud governance, Epic established a steering committee consisting of internal technical leaders and representatives from the AWS account team. Initial discussions focused on cloud maturity levels, best practices, and how AWS could help Epic operate more efficiently.\nFrom there, Epic worked closely with AWS Professional Services and AWS Customer Solutions Manager. They assembled a diverse group of cloud-passionate engineers from various departments to form the Cloud Governance team, also known as the cloud center of excellence (CCoE).\n3. Understand Ownership To implement meaningful change, Epic needed a standardized approach to identify AWS account ownership. The Cloud Governance team identified the person responsible for each account and created real-time communication channels for quick information exchange.\nThis foundation allowed Epic to build and enforce multiple additional controls across the entire cloud environment.\n4. Provision Access The Cloud Governance team focused on providing self-service access to AWS, using a combination of GitHub, Okta, SailPoint, and AWS IAM Identity Center.\nGitHub: stores infrastructure as code (IaC) defining IAM roles and deployment locations. AWS IAM Identity Center: distributes standard IAM roles across all Epic AWS accounts. Okta: provides identities and permission groups for access control. SailPoint: allows account managers to approve or deny access requests. 5. Automate Configuration After implementing self-service access, Epic uses AWS Step Functions to deploy baseline configuration for each account.\nThis includes:\nDefault components like IAM roles Cost-saving measures, such as log expiration for AWS Lambda Security measures, such as enabling Amazon EBS encryption by default in all regions With AWS Step Functions and AWS Lambda, Epic can focus on core configuration without worrying about workflow orchestration, while ensuring all accounts are set up to standard.\n6. Enforce Standards Before deploying policies, Epic needed appropriate mechanisms, so they partnered with governance solution providers.\nEpic forwards events from all accounts to a single point through Amazon EventBridge for analysis and processing. This allows the Cloud Governance team to enforce a minimum tag set to provide information about ownership and costs.\nAutomated mechanisms immediately remove any resources without required tags. Similarly, Epic also applies mechanisms requiring Amazon RDS and Amazon ElastiCache to run on AWS Graviton for cost optimization, but allows exceptions when requested.\nAdditionally, Epic introduced mandatory scanning of infrastructure as code (IaC) changes when pull requests are made in GitHub. If issues are detected, the system automatically comments on the pull request, helping developers fix errors before deployment.\nConclusion By reviewing people, processes, and technology, Epic has made significant progress in their cloud governance journey and achieved clear results.\nBeyond cost savings, Epic now has better resource observability, with nearly 100% of AWS resources tagged in both production and development environments. Continuous access auditing and centralized monitoring have also enhanced security while promoting a positive culture throughout the company.\nFor game developers who want to learn more about smart cloud governance strategies, explore how other companies implement best practices and contact your AWS Representative for support.\nAuthors Gena Gizzi:\nGena Gizzi is a Senior Game Solutions Architect at AWS. She helps customers develop, launch, and scale their games and businesses on AWS. Ryan Rumble:\nRyan Rumble is the Head of Customer Solutions at AWS. In addition to driving Program Management for the team and customers, Ryan specializes in helping companies navigate transformation and maturity on their cloud journey.\n"},{"uri":"https://hviethub.github.io/Internship-Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS First Cloud AI Journey – Project Plan Hello World Team – FPT University – EveryoneCook Date: 30/11/2025\n📥 Download Full Proposal (DOCX)\nTABLE OF CONTENTS 1. BACKGROUND AND MOTIVATION\n1.1 executive summary\n1.2 PROJECT SUCCESS CRITERIA\n1.3 Assumptions\n2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 Technical Architecture Diagram\n2.2 Technical Plan\n2.3 Project Plan\n2.4 Security Considerations\n3. ACTIVITIES AND DELIVERABLES\n3.1 Activities and deliverables\n3.2 OUT OF SCOPE\n3.3 PATH TO PRODUCTION\n4. EXPECTED AWS COST BREAKDOWN BY SERVICES\n5. TEAM\n6. RESOURCES \u0026amp; COST ESTIMATES\n7. ACCEPTANCE\n1.BACKGROUND and motivation 1.1 Executive summary Customer background\nThe customer is a startup focused on building a modern social network platform where users can share cooking recipes, upload food photos, exchange culinary experiences, and explore meals recommended by AI. The organization aims to deliver a highly interactive platform capable of serving a large and growing user base.\nBusiness and technical objectives \u0026ndash; drivers for moving to the AWS cloud\nEnable rapid development and deployment using AWS managed services Ensure high scalability as the user base and media storage grow Provide a reliable, low-latency environment for AI computation and content delivery Reduce upfront infrastructure cost and move toward a pay-as-you-go model Improve data security, backup, and compliance through AWS-native capabilities Use cases\nUsers upload recipes, photos, and cooking videos to the platform System recommends dishes using AI based on available ingredients provided by the user Users interact socially through liking, commenting, sharing, and following AI processes text and images to generate recipe suggestions Admins manage content, monitor platform activity, and track performance analytics To meet the customer\u0026rsquo;s objectives of building a scalable social cooking platform with AI-powered recipe recommendations, the partner will deliver a full end-to-end cloud implementation on AWS. The services provided include:\nCloud Architecture Design: Define a secure, highly scalable, serverless architecture using AWS best practices (Route 53, API Gateway, Lambda, DynamoDB, S3, CloudFront, Cognito) AI Integration: Implement AWS Bedrock (Claude 3.5 Sonnet) for intelligent recipe suggestions, image analysis, and natural language processing features Infrastructure Deployment: Build and deploy all backend, frontend, authentication, and data layers using Infrastructure as Code (IaC) with fully automated CI/CD pipelines Security \u0026amp; Compliance: Configure IAM roles, encryption (KMS), WAF, logging, monitoring, and compliance guardrails to ensure platform security Observability Setup: Enable CloudWatch dashboards, alarms, X-Ray tracing, and log centralization for real-time monitoring and performance insights DevOps \u0026amp; Automation: Implement automated build/deploy workflows via GitLab + Amplify, operational pipelines, and auto-scaling configurations Performance Optimization: Configure CDN caching, DynamoDB capacity scaling, search indexing, and asynchronous SQS-based background processing Knowledge Transfer \u0026amp; Documentation: Provide technical documentation, best practices, architectural guides, and handover training to the customer\u0026rsquo;s engineering team 1.2 Project Success Criteria Project Success Criteria\nSystem availability ≥ 99.9% uptime across all production services (API Gateway, Lambda, DynamoDB, CloudFront).\nPage load time \u0026lt; 2.5 seconds for the main user interface delivered through CloudFront and Amplify.\nAPI response time \u0026lt; 300 ms for 90% of all user-facing API requests under normal traffic conditions.\nAI processing latency \u0026lt; 5 seconds for recipe suggestions generated by AWS Bedrock.\nUser authentication success rate ≥ 98% with Cognito handling registration, login, and email verification.\nZero critical security vulnerabilities after security review and WAF rules deployment.\nData durability of 99.999999999% (11 nines) ensured through S3 object storage and DynamoDB.\nScalability to support 10,000+ concurrent users without degradation in performance due to serverless infrastructure.\nOperational cost control within target budget: monthly AWS usage not exceeding $200 for production.\nImage upload \u0026amp; processing success rate ≥ 99%, supported by S3, Lambda Workers, and SQS.\nSearch performance under 1 second (if OpenSearch is enabled) for recipe/content search queries.\nMonitoring coverage of 100% critical services using CloudWatch dashboards, alarms, and X-Ray tracing.\nCI/CD deployment time \u0026lt; 5 minutes via GitHub → Amplify and IaC automation.\nZero data loss events, ensured by DynamoDB PITR and S3 versioning.\n1.3 Assumptions The customer will provide full access to their domain registrar (Hostinger) to configure DNS delegation to Route 53.\nThe customer will provide valid AWS account access with Administrator privileges for deployment and configuration.\nAll required AWS services (Amplify, API Gateway, Lambda, DynamoDB, CloudFront, Cognito, Bedrock, SES) are available and supported in the chosen region.\nSES will be successfully moved out of the sandbox and approved for production email sending.\nThird-party integrations (GitHub for CI/CD, external email clients, image upload sources) will remain available and stable.\nThe development team will maintain source code quality and follow the architectural guidelines provided by the partner.\nThe customer will provide timely feedback and approvals during design, testing, and deployment phases.\nDependencies\nReliable internet connectivity is required for all users accessing the web application and APIs.\nThe system depends on AWS Bedrock (Claude 3.5 Sonnet) for AI recipe generation and may experience performance fluctuations if the model becomes rate-limited.\nImage upload and processing workflows depend on S3, Lambda, and SQS processing reliability.\nIf OpenSearch is enabled, search features rely on the availability of the OpenSearch domain.\nGitHub Actions and Amplify depend on GitHub service availability.\nConstraints\nThe project will be fully deployed in a single AWS region, which may impact latency for users outside the region.\nThe solution is designed using serverless patterns; custom EC2-based workloads are outside the project scope.\nSES domain reputation may affect email deliverability during initial weeks.\nOpenSearch is deployed as a single-node cluster for cost efficiency, which means no high availability for search indexing.\nThe system must stay within the customer\u0026rsquo;s cost target (\u0026lt; $200/month), limiting the use of large compute resources.\nRisks\nSES production approval may be delayed, impacting user onboarding emails and notifications.\nIf traffic grows unexpectedly, DynamoDB provisioned capacity may throttle without timely scaling adjustments.\nAI model cost or latency changes by AWS may impact application performance or cost control.\nMisconfigured CloudFront caching could lead to higher latency or increased data transfer cost.\nAny incorrect IAM configuration could lead to security risks or service disruption.\nCustomer team turnover or lack of DevOps skills may slow down future maintenance or deployments.\n2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The proposed solution architecture for the AI-powered cooking social network platform is designed using a fully serverless and scalable AWS cloud-native stack. The architecture ensures high availability,security, and seamless integration between the web frontend, backend APIs, authentication, data storage, and AI recommendation services. Below is a description of the key components and how data flows across the system: 1. Network \u0026amp; Edge Layer\nAmazon Route 53 (6\u0026ndash;7)\nProvides DNS routing for the custom domain used by the platform. Incoming HTTPS requests from users are resolved and forwarded to CloudFront. Amazon CloudFront (9)\nActs as a global CDN distributing frontend content with low latency while caching static files. AWS WAF (8)\nProtects the application from common web exploits such as SQL injection, XSS, and bot attacks. 2. Frontend Hosting \u0026amp; Deployment\nAWS Amplify Hosting (4)\nHosts and deploys the Next.js frontend application. Integrated with GitLab CI/CD (3) for automated deployments from the development workflow. 3. Application Layer\nAmazon Cognito (10)\nHandles user authentication and authorization, supporting email/password and social logins. Amazon API Gateway (11)\nServes as the main entry point for backend APIs, exposing REST endpoints used by the frontend. AWS Lambda (12, 15)\nContains the backend business logic, including:\nuser management\npost and recipe operations\ningredient analysis\nconnecting to Bedrock for AI recommendations\nThis serverless architecture ensures automatic scaling and pay-per-use cost efficiency.\n4. AI Recommendation Layer\nAmazon Bedrock (16\u0026ndash;17)\nProvides generative AI capabilities to suggest recipes based on user-provided ingredients.\nLambda invokes Bedrock models (e.g., Claude, Titan) to:\nanalyze ingredient lists\ngenerate recipe suggestions\nclassify food categories\noptimize cooking steps.\n5. Data Storage Layer\nAmazon DynamoDB (13)\nStores structured application data such as:\nuser profiles\nposts/recipes\nlikes \u0026amp; comments\ningredient metadata.\nAmazon S3 (14)\nStores unstructured data:\nrecipe images\nuser-uploaded food photos\nstatic content.\nAn S3 bucket is integrated with CloudFront via OAI for secure access.\n6. Observability \u0026amp; Security Layer\nAmazon CloudWatch (Logs \u0026amp; Metrics)\nMonitors Lambda performance, API Gateway access logs, and system metrics.\nAWS X-Ray\nPerforms distributed tracing for API calls and debugging.\nIAM\nDefines permission boundaries between API, Lambda functions, Bedrock, DynamoDB, and S3.\nAmazon SES\nSends verification emails, notifications, and password recovery messages.\nAmazon SNS\nHandles system-level alerts and asynchronous messaging.\n7. Deployment \u0026amp; Infrastructure Management\nAWS CDK (1\u0026ndash;2)\nUsed by developers to define and provision the entire infrastructure via CloudFormation templates.\nEnsures consistent, reproducible, version-controlled deployments. 2.2 Technical Plan Partner will develop Infrastructure-as-Code (IaC) automation using AWS CDK (Cloud Development Kit) with TypeScript/Python to provision the entire cloud environment. This approach ensures quick, consistent, and repeatable deployments across multiple AWS accounts and environments (dev, staging, production). All resources such as API Gateway, Lambda functions, DynamoDB tables, S3 buckets, Cognito user pools, Bedrock integration policies, and CloudFront distributions will be fully automated via IaC.\nApplication build and deployment processes for the frontend (Next.js) will be automated using AWS Amplify Hosting, integrated with GitLab pipelines. Backend components will be deployed through CDK pipelines to ensure controlled, versioned, and repeatable releases.\nSome additional configuration such as custom domain setup, Route 53 DNS changes, SSL/TLS certificate issuance, and IAM permission approvals may require customer review and approval. These changes will follow the customer's existing change management process, including scheduled maintenance windows and documented approvals from the security/compliance teams.\nAll critical paths, including authentication flows, AI recipe suggestion APIs, data persistence logic, and image upload workflows, will undergo extensive test coverage. Automated tests (unit, integration, and API-level) will be executed in CI/CD pipelines, and manual validation will be performed in the staging environment before production deployment.\n2.3 Project Plan [Partner] will adopt the Agile Scrum framework across eight 2-week sprints. Stakeholders from the team are required to participate in Sprint Reviews and Sprint Retrospectives to ensure alignment and continuous improvement.\nThe proposed team responsibilities are as follows:\nProduct Owner: Define user stories, prioritize backlog, and ensure the product meets user needs.\nScrum Master**:** Facilitate Scrum ceremonies, remove blockers, and maintain team productivity.\nDevelopment Team: Implement features, conduct unit testing, and collaborate on integration.\nAI/ML Specialist: Develop and fine-tune the AI recommendation engine that suggests recipes based on user-provided ingredients.\nUI/UX Designer: Design intuitive interfaces and ensure a smooth user experience on both web and mobile platforms.\nQA/Testers**:** Validate feature functionality, conduct regression testing, and ensure system reliability.\nCommunication cadences will be established as follows:\nDaily Stand-ups**:** 15-minute meetings for progress updates and immediate blockers.\nSprint Planning**:** At the start of each sprint to prioritize tasks.\nSprint Review: At the end of each sprint to showcase completed features to stakeholders.\nSprint Retrospective**:** Following each sprint review to identify improvements for the next sprint.\nKnowledge transfer sessions will be conducted by the senior developers and AI specialists to ensure team members understand system architecture, AI integration, and deployment procedures.\n2.4 Security Considerations Partner will implement security best practices across the following five categories to ensure the confidentiality, integrity, and availability of the platform:\nAccess Enable Multi-Factor Authentication (MFA) for all user and administrative accounts.\nImplement role-based access control (RBAC) to limit permissions based on user roles (e.g., admin, moderator, contributor).\nEnforce strong password policies and periodic password rotation.\nInfrastructure Deploy the application on secure, managed cloud services (e.g., AWS) following AWS security best practices.\nUse Virtual Private Cloud (VPC), network segmentation, and security groups to isolate resources.\nRegularly patch operating systems and containerized services to mitigate vulnerabilities.\nData Encrypt all data at rest using AWS KMS-managed keys and data in transit using TLS/HTTPS.\nImplement data classification to protect sensitive user information (e.g., email, profile data, dietary preferences).\nApply secure data storage and backup procedures to ensure availability and integrity.\nDetection Enable AWS CloudTrail and AWS Config to monitor API activity and resource configurations.\nDeploy logging and alerting mechanisms to detect unusual or suspicious activities in real time.\nConduct periodic vulnerability scanning and penetration testing on the platform.\nIncident Management Establish a formal incident response plan including detection, containment, remediation, and communication.\nMaintain audit trails and logs to support forensic investigation if a security event occurs.\nEnsure [Customer] shares regulatory control validations to help [Partner] meet all compliance requirements (e.g., GDPR, local privacy regulations).\nBy adhering to these measures, Partner ensures that the social cooking platform remains secure, compliant, and resilient against potential threats.\n3. Activities AND Deliverables 3.1 Activities and deliverables [Provide project milestones with timeline and respective deliverables, corresponding to the items and activities described in the Scope of Work / Technical Project Plan section. Indicate plans on how to govern the project/ change management; communication plans; transition plans]\nProject Phase Timeline Activities Deliverables/Milestones Total man-day Infrastructure Setup Week 1-2 - Learn all aws service\n- Practice Lab - Worklog 2 week Project Foundation \u0026amp; Infrastructure Setup Week 3 - Initialize monorepo structure\n- setup development environment - Initialize Git repository and CI/CD\n- Initialize CDK project structure\n-Create environment configuration system\n-Setup CDK deployment scripts\n- Setup Git repository with CI/CD pipelines and branch protection\n- Configure local testing scripts and Git hooks for code quality\n- Set up AWS CDK project structure with proper organization for infrastructure as code\n- Implement centralized configuration management for dev, staging, and prod environments\n1 week -DNS Infrastructure (Route 53 Hosted Zone) - DNS Stack\n- Side Quest: CloudFront yêu cầu ACM certificate ở us-east-1, nhưng stack chính deploy ở ap-southeast-1\nWeek 4 ,5 - Create a Public Hosted Zone\n- Configure name server delegation\n- Architecture Design\n- Create DNS Stack in CDK project\n- Connect DNS , User Route 53 Alias targeting for AWS - managed - Request ACM certificates, Configure DNS validation in Route 53\n- Configure MX records for email , add SPF,DKIM,DMARC for email authentication\n- Create DNS structure , Validate DNS\n-npm run cdk deploy EveryoneCook-dev-Certificate - Domain \u0026amp; Hosted Zone Setup\n- Deploy DNS Stack\n- Route 53 Hosted Zone Status\n- Create Public Hosted Zone \u0026amp; NS Delegation Plan\n- ACM Certificates Automation\n- Test DNS\n- Link DNS to AWS Resources\n- Monitoring\n- Deploy Certificate Stack 2 weeks Structure Core Stack Week 6,7 - Initialize Core Stack for DynamoDB, S3, CloudFront, and OpenSearch infrastructure\n- Create DynamoDB table with Provisioned Mode and Auto-Scaling for cost optimization\n- Implement Global Secondary Indexes for diverse access patterns\n- Configure KMS encryption and security settings for DynamoDB\n- Create all 4 S3 buckets (content, logs, incoming emails, CDN logs) with Intelligent-Tiering for cost optimization\n- Configure CloudFront CDN with compression and caching optimization\n- Setup signed URLs for private content (avatars, backgrounds)\n- Create OpenSearch domain for advanced search with cost optimization\n- Create CoreStack class\n- Implement DynamoDB Single Table with cost optimization\n- Create 5 GSI indexes\n- Setup encryption and security for DynamoDB - S3 Storage, CloudFront CDN,OpenSeach Domain\n- Deploy Core Stack\n2 week Authentication Stack Week 8 - Initialize Authentication Stack for Cognito User Pool\n- Create Cognito User Pool with production-grade security settings\n- Setup SES for production email sending with Route 53 DNS automation\n- Implement Lambda triggers for Cognito lifecycle events\n- Cognito User Pool Setup\n- Implement Cognito User Pool with production settings\n- Configure SES email integration (Production mode)\n- Setup Cognito Lambda triggers\n1 week Backend Stack (API Gateway + Lambda ) Week 9,10 - Create API Gateway REST API with production settings and Cognito authorizer\n- Configure Cognito User Pool authorizer for API Gateway\n- Enable API Gateway caching for production to improve performance and reduce Lambda invocations\n- Enable request validation at API Gateway level to reject invalid requests early\n- Enable compression for API responses to reduce data transfer costs\n- Configure API Gateway custom domain for Everyone Cook project\n- Setup API Router Lambda directory structure\n- Implement routing logic for API Gateway requests\n- Deploy API Router Lambda to AWS and implement JWT validation for Cognito tokens\n- Setup Auth \u0026amp; User module directory structure\n- Create BackendStack class\n- Create API Gateway REST API\n- Setup Cognito Authorizer\n- Configure API Gateway caching\n- Configure API Gateway request validation\n- Enable API Gateway compression\n- Configure API Gateway custom domain\n- Create API Router Lambda structure\n- Implement API Router handler\n- Deploy API Router Lambda + Implement JWT Validation\n- Create Auth \u0026amp; User module structure\n- Implement authentication handlers\n- Implement user profile handlers,…\n- Social Module Lambda\n2 weeks Frontend Stack + Fix bug, Tester Week 11,12 - Foundation\n- Authentication\n- User Profile Management\n- Privacy Setting Pages\n- Avatar Upload Component\n- Recipe Management\n- AI Recipe Suggestion\n- Social Feature\n- Search Discovery\n- Testing and Fix Bug\n- Functional profile view page\n- Login, signup, logout, password reset, and security middleware\n- Profile editing with validation\n- Avatar upload with S3 integration\n- Privacy settings interface\n2 weeks 3.2 OUT OF SCOPE Real-time Messaging / Chat System\nPrivate or group chat\nReal-time messaging infrastructure (WebSocket, SignalR, Firebase Realtime DB, etc.)\nMessage history storage \u0026amp; encryption\nFriends / Social Graph Management\nFriend requests, following/followers\nUser-to-user connection graph\nActivity feed, notifications tied to friend actions\nReal-time Voice \u0026amp; Video Calling\n1-to-1 or group voice call\nVideo call, screen sharing\nWebRTC signaling servers \u0026amp; TURN/STUN infrastructure\nAdvanced Social Interaction\nIn-app messaging reactions\nTyping indicators, online/offline status\nRead receipts, presence tracking\n3.3 PATH TO PRODUCTION 1. Project Foundation \u0026amp; Infrastructure\n- Initialize project structure\n- Set up core infrastructure baseline\n- Configure Route 53 Hosted Zone (DNS Stack)\n2. Cross-Region Certificate (Side Quest)\n- Handle CloudFront requirement for ACM certificate in us-east-1\n- Sync certificate usage with main stack in ap-southeast-1\n3. Core Application Stacks\n- Core Stack: Shared resources / environment setup\n- Authentication Stack: User auth, Cognito, permissions\n- Backend Stack: API Gateway + Lambda functions\n4. Frontend Deployment\n- Deploy frontend (S3 + CloudFront)\n- Bug fixes \u0026amp; QA testing\n4. EXPECTED AWS COST BREAKDOWN BY SERVICES Target workload: 100-500 Monthly Active Users (MAU)\nAverage Lambda duration: 200ms per invocation\nDynamoDB peak activity: ~8 hours per month\nS3 to CloudFront data transfer is FREE (same region)\nAll services leverage AWS Free Tier where applicable (Lambda 1M requests, SQS 1M requests, Cognito 50K MAU, Amplify 1000 build minutes)\nAPI Gateway caching enabled at 0.5GB ($14.60/month) - can be disabled to reduce costs\nCloudFront WAF removed to optimize costs (~$9/month savings), Shield Standard provides DDoS protection\nBedrock uses on-demand pricing with Claude 3 Haiku (lowest cost Anthropic model)\nhttps://calculator.aws/#/estimate?id=7a8833402a63e273357ddc71071bfc2cdce4be2c\n5. TEAM Partner Executive Sponsor\nName Title Description Email / Contact Info Nguyen Gia Hung Director of FCJ Vietnam Training Program As the Executive Sponsor, you are responsible for the overall oversight of the FCJ internship program. Ensure the project delivers learning value and adheres to AWS technical and career goals hunggia@amazon.com Project Stakeholders\nName Title Stakeholder for Email / Contact Info Van Hoang Kha Support Teams is the Executive Assistant responsible for overall oversight of the FCJ internship program Khab9thd@gmail.com Partner Project Team\nName Title Role Email / Contact Info Pham Minh Hoang Viet Leader Project Manager vietpmhse181851@fpt.edu.vn Nguyen Van Truong Member DevOps truongnvse182034@fpt.edu.vn Huynh Duc Anh Member Cloud Engineer anhhdse183114@fpt.edu.vn Nguyen Thanh Hong Member Tester hongntse183239@fpt.edu.vn Nguyen Qui Duc Member Frontend ducnqse182087@fpt.edu.vn Project Escalation Contacts\nName Title Role Email / Contact Info Pham Minh Hoang Viet Leader Project Manager vietpmhse181851@fpt.edu.vn 6. Resources \u0026amp; cost estimates Resource Responsibility Rate (USD) / Hour Solution Architects [[1]]{.mark} Architecture design, AWS service selection, security review, cost optimization $150 Engineers [[2]]{.mark} Frontend (Next.js), Backend (Lambda/Node.js), Infrastructure (CDK), Testing $100 Other . DevOps [1] CI/CD setup, monitoring, deployment automation $80 Project Phase Solution Architects Engineers Other\n(DevOps)\nTotal Hours Discovery \u0026amp; Requirements 16 24 8 48 Architecture Design 40 16 8 64 Development 16 200 40 256 Testing \u0026amp; QA 8 40 16 64 Deployment \u0026amp; Go-Live 8 24 24 56 Documentation \u0026amp; Training 8 16 8 32 Total Hours 96 320 104 520 Total Cost $14,400 | $32,000 $8,320 | $54,720 Monthly AWS Infrastructure Cost\nBased on AWS Pricing Calculator estimate for 100-500 MAU:\nService Monthly Cost (USD) Description Amazon DynamoDB $13.06 Single-table design, 5 GSIs, provisioned capacity Amazon S3 $0.84 2 buckets, Intelligent-Tiering Amazon CloudFront $1.44 CDN, Price Class 200 Amazon Cognito $5.00 User authentication AWS Lambda $0.00 13 functions (Free Tier) Amazon API Gateway $20.65 REST API with 0.5GB cache Amazon SQS $0.00 8 queues (Free Tier) Amazon SES $0.02 Transactional emails AWS KMS $2.00 2 customer managed keys AWS WAF $10.00 Web ACL, 5 rules Amazon CloudWatch $21.25 Metrics, dashboards, alarms, logs Amazon Route 53 $0.93 DNS hosted zone AWS Amplify $4.58 Frontend hosting (Next.js) Amazon Bedrock $64.80 Claude 3 Haiku AI Total ~$144.54 Per month Cost Summary\nCost Type Amount (USD) Notes One-time Development Cost $54,720 Resource hours × rates Monthly AWS Infrastructure ~$145 Based on 100-500 MAU Annual AWS Infrastructure ~$1,740 Monthly × 12 Year 1 Total Cost ~$56,460 Development + 12 months AWS Cost Contribution Distribution\nParty Contribution (USD) % Contribution of Total Customer $54,720 100% Partner $0 $0 AWS $0 $0 7. Acceptance Since this project is currently at the presentation stage and has not yet been formally evaluated by a customer, the following acceptance process is proposed for future delivery phases:\n7.1 Acceptance Criteria (Proposed) A deliverable will be considered acceptable when it meets the following criteria:\nFunctional features work as specified (authentication, recipe management, social features, AI functions).\nAll APIs respond correctly and integrate with AWS services (Lambda, API Gateway, DynamoDB, S3).\nSecurity requirements are met (JWT verification, HTTPS, access control, data encryption).\nUI works as expected on supported devices.\nNo critical errors appear during test execution.\n7.2 Acceptance Process Review period: 8 business days for evaluation and testing.\nIf accepted → Deliverable is signed off.\nIf issues are found → A rejection notice will be issued with feedback.\nFixes will be applied and a revised version will be resubmitted for review.\nIf no response is received by the end of the review period → Deliverable is deemed accepted.\nAfter completing each milestone, the team submits the deliverables and documentation.\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - LMR: Build and Deploy a Global Serverless Application with Amazon DynamoDB Practice: + Module 1: Deploy the backend resources + Module 2: Explore Global Tables + Module 3: Interact with the Globalflix Interface - Global Tables Discussion Topics - LEDA: Build a Serverless Event Driven Architecture with DynamoDB - Practice: + Lab 1: Connect the pipeline + Lab 2: Ensure fault tolerance and exactly once processing\n+ Cleanup resource 11/10/2025 11/10/2025 https://000039.awsstudygroup.com/ 3 - LGME: Modeling Game Player Data with Amazon DynamoDB - Practice: + Plan data model + Core usage: user profiles and games + Find open games + Join and close games + View past games + Summary \u0026amp; Cleanup - LDC: Design Challenges 11/11/2025 11/11/2025 https://000039.awsstudygroup.com/ 4 - Cost and performance analysis with AWS Glue and Amazon Athena - Practice: + Preparing the database + Building a database + Database Check + Data in the Table + Cost + Tagging and Cost Allocation + Usage + Cleanup resource 11/12/2025 11/12/2025 https://000040.awsstudygroup.com/ 5 - Work with Amazon DynamoDB - Practice: + Manage using AWS Management Console + Use AWS CloudShell + Configure AWS CLI + Getting started with Python and DynamoDB - Clean up resource 11/13/2025 11/13/2025 https://000060.awsstudygroup.com/ 6 - Building a Datalake with Your Data - Practice: + Preparing Data + Data Ingestion with AWS Glue + Query with Athena + Visualization with QuickSight + Resource Cleanup 11/14/2025 11/14/2025 https://000070.awsstudygroup.com/ Week 10 Achievements: Successfully built and deployed a global serverless application using DynamoDB Global Tables, gaining hands-on experience with multi-Region architecture and event-driven pipelines with fault tolerance and exactly-once processing.\nStrengthened DynamoDB data-modeling skills through game player data modeling, covering user profiles, game sessions, open/closed games, and historical queries.\nGained deeper experience with AWS Glue \u0026amp; Athena by performing database preparation, ETL operations, querying, cost analysis, tagging, and cost allocation for analytics workloads.\nImproved DynamoDB operational skills using Management Console, CloudShell, AWS CLI, and Python SDK (boto3).\nBuilt a full data lake pipeline end-to-end: data preparation, Glue ingestion, Athena query optimization, and QuickSight visualization.\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - DevOps on AWS (Event) 11/18/2025 11/18/2025 https://luma.com/39t066sy 3 - Analytics on AWS workshop - Practice: + Create S3 bucket + Create Kinesis Firehose + Generate Dummy Data + Create IAM Role + Create AWS Glue Crawlers + Verify newly created tables in catalog 11/19/2025 11/19/2025 https://000072.awsstudygroup.com/ 4 - Analytics on AWS workshop - Practice: + Transform Data with AWS Glue (interactive sessions) + Transform Data with AWS Glue Studio (graphical interface) + Transform Data with AWS Glue DataBrew + Transform Data with EMR 11/20/2025 11/20/2025 https://000072.awsstudygroup.com/ 5 - Analytics on AWS workshop - Practice: + Analysis with Athena + Analysis with Kinesis Data Analytics + Visualize in Quicksight + Serve with Lambda - Warehouse on Redshift - Clean up 11/21/2025 11/21/2025 https://000072.awsstudygroup.com/ 6 - Get started with Quick Sight - Practice: + Sign up for QuickSight + Build dashboard + Dashboard Improvements + Create Interactive Dashboard + Clean up resources 11/22/2025 11/22/2025 https://000073.awsstudygroup.com/ Week 11 Achievements: Completed AWS Analytics workshops, gaining hands-on experience in building end-to-end data pipelines.\nPracticed data ingestion and preparation:\nCreated S3 buckets, Kinesis Firehose streams, sample datasets, IAM roles, and Glue crawlers.\nPerformed data transformation using multiple AWS tools: Glue (interactive sessions), Glue Studio (graphical interface), Glue DataBrew, and EMR.\nConducted data analysis and visualization:\nQueried data with Athena, analyzed streams with Kinesis Data Analytics, visualized insights in QuickSight, and served results via Lambda.\nWorked with Redshift for data warehousing tasks and ensured proper resource cleanup.\nGained proficiency in QuickSight dashboards: registration, creation, improvement, interactive dashboards, and resource management.\n"},{"uri":"https://hviethub.github.io/Internship-Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Find bugs and fix bugs in the project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Find bugs and fix bugs in the project - Test funtion 11/24/2025 11/24/2025 3 - Find bugs and fix bugs in the project - Test funtion 11/25/2025 11/25/2025 4 - Find bugs and fix bugs in the project - Test funtion 11/26/2025 11/26/2025 5 - Find bugs and fix bugs in the project - Test funtion 11/27/2025 11/27/2025 6 - Find bugs and fix bugs in the project - Test funtion 11/28/2025 11/28/2025 Week 12 Achievements: Found and fixed bugs in the project "},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.3-cdk-bootstrap/","title":"CDK Bootstrap","tags":[],"description":"","content":"Overview AWS CDK Bootstrap tạo các resources cần thiết trong AWS account để deploy CDK applications. Bạn chỉ cần chạy bootstrap một lần cho mỗi account/region.\nWhat is CDK Bootstrap? CDK Bootstrap tạo:\nS3 Bucket: Lưu trữ CDK assets (Lambda code, Docker images) ECR Repository: Lưu trữ Docker images (nếu dùng containers) IAM Roles: Permissions cho CloudFormation và CDK SSM Parameters: Lưu trữ bootstrap version Prerequisites Đảm bảo bạn đã:\n✅ Cài đặt AWS CLI và CDK CLI ✅ Configure AWS credentials ✅ Có quyền AdministratorAccess (hoặc tương đương) Bootstrap Command 1. Check Current Bootstrap Status\n# Check if already bootstrapped aws cloudformation describe-stacks \\ --stack-name CDKToolkit \\ --region us-east-1 # If not bootstrapped, you\u0026#39;ll get an error 2. Run Bootstrap\n# Navigate to infrastructure directory cd infrastructure # Bootstrap for us-east-1 region cdk bootstrap aws://YOUR-ACCOUNT-ID/us-east-1 # Replace YOUR-ACCOUNT-ID with your actual AWS account ID # Get account ID: aws sts get-caller-identity --query Account --output text Example:\n# Get your account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Bootstrap cdk bootstrap aws://$ACCOUNT_ID/us-east-1 Screenshot: Terminal showing CDK bootstrap in progress\n3. Wait for Completion\nBootstrap takes 2-3 minutes. You\u0026rsquo;ll see:\n⏳ Bootstrapping environment aws://123456789012/us-east-1... CDKToolkit: creating CloudFormation changeset... ✅ Environment aws://123456789012/us-east-1 bootstrapped. Screenshot: Terminal showing successful bootstrap\nVerify Bootstrap 1. Check CloudFormation Stack\n# List CDK bootstrap stack aws cloudformation describe-stacks \\ --stack-name CDKToolkit \\ --region us-east-1 \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; # Should return: CREATE_COMPLETE 2. Check S3 Bucket\n# List CDK assets bucket aws s3 ls | grep cdk # Should show: cdk-hnb659fds-assets-ACCOUNT-ID-us-east-1 Screenshot: CloudFormation console showing CDKToolkit stack\n3. Check IAM Roles\n# List CDK roles aws iam list-roles | grep cdk # Should show roles like: # - cdk-hnb659fds-cfn-exec-role-ACCOUNT-ID-us-east-1 # - cdk-hnb659fds-deploy-role-ACCOUNT-ID-us-east-1 # - cdk-hnb659fds-file-publishing-role-ACCOUNT-ID-us-east-1 Bootstrap for Multiple Regions (Optional) If you plan to deploy to multiple regions:\n# Bootstrap additional regions cdk bootstrap aws://$ACCOUNT_ID/ap-southeast-1 cdk bootstrap aws://$ACCOUNT_ID/eu-west-1 # Note: For this workshop, we only use us-east-1 Understanding Bootstrap Resources S3 Bucket:\nStores Lambda deployment packages Stores CloudFormation templates Versioned for rollback support IAM Roles:\ncfn-exec-role: CloudFormation execution role deploy-role: CDK deployment role file-publishing-role: Asset publishing role image-publishing-role: Docker image publishing role SSM Parameters:\n/cdk-bootstrap/hnb659fds/version: Bootstrap version Bootstrap Cost Free Tier:\nS3 bucket: First 5GB free IAM roles: Free SSM parameters: Free Ongoing Costs:\nS3 storage: ~$0.023/GB/month Typical usage: \u0026lt; 1GB = ~$0.02/month Troubleshooting Issue: Access Denied\nError: Need to perform AWS calls for account XXX, but no credentials found Solution:\n# Reconfigure AWS credentials aws configure # Or set environment variables export AWS_ACCESS_KEY_ID=your-key export AWS_SECRET_ACCESS_KEY=your-secret export AWS_DEFAULT_REGION=us-east-1 Issue: Already Bootstrapped\nError: Stack [CDKToolkit] already exists This is OK! Your account is already bootstrapped. You can:\nSkip this step Or update bootstrap: cdk bootstrap --force Issue: Insufficient Permissions\nError: User is not authorized to perform: cloudformation:CreateStack Solution:\nEnsure your IAM user has AdministratorAccess Or add specific permissions for CDK bootstrap Issue: Wrong Region\nError: Stack is in different region Solution:\n# Specify region explicitly cdk bootstrap aws://$ACCOUNT_ID/us-east-1 --region us-east-1 Bootstrap Customization (Advanced) For production, you might want to customize bootstrap:\n# Custom bootstrap with specific bucket name cdk bootstrap \\ --toolkit-stack-name CustomCDKToolkit \\ --qualifier custom \\ aws://$ACCOUNT_ID/us-east-1 Note: For this workshop, use default bootstrap.\nClean Up Bootstrap (Optional) To remove bootstrap resources (only if you\u0026rsquo;re done with CDK):\n# Delete CDK bootstrap stack aws cloudformation delete-stack \\ --stack-name CDKToolkit \\ --region us-east-1 # Delete S3 bucket (must be empty first) BUCKET_NAME=$(aws s3 ls | grep cdk | awk \u0026#39;{print $3}\u0026#39;) aws s3 rm s3://$BUCKET_NAME --recursive aws s3 rb s3://$BUCKET_NAME Warning: Only do this if you\u0026rsquo;re completely done with CDK in this account/region!\nNext Steps Once bootstrap is complete, proceed to Configure Infrastructure Stacks to set up your CDK stacks configuration.\n"},{"uri":"https://hviethub.github.io/Internship-Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Press Start on AWS Training for Game Development By Srividhya Pallay, Kenneth Tang, and Michael Orman — 03/03/2025\nCategories: Amazon Aurora, Amazon DynamoDB, Amazon EC2, Amazon Elastic Kubernetes Service (Amazon EKS), Amazon GameLift, Amazon Kinesis, Amazon Redshift, Amazon Simple Storage Service (Amazon S3), AWS Training and Certification, Best Practices, Game Development, Games, Technical How-to\nGame developers often face a common challenge: building cloud expertise while staying focused on creating games.\nModern games require sophisticated cloud infrastructure to serve millions of concurrent players, manage real-time multiplayer experiences, and deliver seamless global gameplay — making cloud expertise a critical factor in game development.\nThis article compiles essential training resources from foundational courses to hands-on workshops and industry-recognized certifications. Whether you are:\nAn indie developer looking to scale your first game A technical artist needing cloud-based rendering solutions Or a studio engineer designing large-scale game infrastructure → You\u0026rsquo;ll find a suitable learning path for every stage of your cloud game development journey.\nThis article covers 4 main sections: AWS Skill Builder training through digital courses and hands-on labs AWS Workshops and AWS Immersion Days to deploy real-world game solutions AWS Certifications relevant to game technology needs Additional resources including GitHub repositories, architecture guides, and training programs AWS Skill Builder Trainings In AWS Skill Builder, AWS for Games content provides a comprehensive set of digital courses and hands-on training. Courses range from AWS fundamentals to specialized AWS services for games, suitable for both beginners and advanced practitioners.\nRecommended Learning Path: Getting Started with AWS for Games – Part I\n→ Foundation course introducing AWS services relevant to games and benefits of AWS Cloud in the gaming industry\nGetting Started with AWS for Games – Part II\n→ Advanced: game server management, game analytics \u0026amp; player management, LiveOps, infrastructure best practices\nGame Server Hosting on AWS\n→ Discussion of game server hosting on AWS Cloud, including performance, operations, security; emphasis on Amazon GameLift\nAWS for Games Containers Principles\n→ Using Amazon ECS \u0026amp; Amazon EKS to run microservices architecture for games\nAWS for Games Learning Plan: Databases\n→ Integrating databases \u0026amp; storage with DynamoDB, Aurora, ElastiCache Serverless, S3, Kinesis across multiple game genres\nAWS for Games Learning Plan: Game Servers\n→ 5-course series: from basic server concepts → advanced (GameLift, containerization, multi-region, DevOps)\nCloud Game Development – Knowledge Badge Readiness Path\n→ 9 courses covering the entire game development pipeline; validates expertise in building game environments on AWS cloud\nAWS Workshops For those who prefer learning through hands-on experience, AWS Workshops provide practical environments with detailed guidance.\nAmazon GameLift Workshops Start a Full Stack Game with GameLift\n→ Build multiplayer, session-based games with FlexMatch \u0026amp; FleetIQ\nAWS Game Tech Workshop\n→ GameLift + optimized build farm for Unity/Unreal\nAWS Game Backend Framework Workshop\n→ Flexible backend for Unreal, Unity, Godot\nAmazon GameLift Testing Toolkit Workshop\n→ Hands-on deployment \u0026amp; usage of Testing Toolkit\nDatabase \u0026amp; Analytics Workshops Online Game Inventory with Amazon Serverless\n→ Use Aurora Serverless, Lambda, API Gateway to build inventory system\nAmazon Redshift for Gaming Industry\n→ Near real-time player data analysis with Redshift\nAnalytics \u0026amp; AI/ML for Games\n→ Build analytics pipeline to improve engagement\nIdeation Workshops Upgrade Your Game Storyboard with Multi-Modal Prompt Chaining\n→ Use Generative AI to develop storyboards from initial concepts\nAWS Certifications AWS Certifications validate technical expertise in cloud domains. These are widely recognized credentials demonstrating the ability to design \u0026amp; deploy solutions on AWS.\nFor game developers, these 3 certifications are particularly relevant:\nAWS Certified Solutions Architect – Associate → Foundation certification: designing distributed systems on AWS (EC2, S3, VPC, RDS, Route 53, IAM, etc.)\nAWS Certified Developer – Associate → Focus on services popular with games: Lambda, DynamoDB, API Gateway, S3, CloudFormation; CI/CD, serverless, containerization\nAWS Certified Advanced Networking – Specialty → Design \u0026amp; deploy complex networks: VPC design, Transit Gateway, Direct Connect, Global Accelerator, Route 53; focus on low-latency multiplayer, global matchmaking, real-time data synchronization\nAdditional Resources AWS Game Tech GitHub\n→ Server samples, reference architectures, Unity/Unreal integrations, detailed tutorials\nAWS Game Tech Resources\n→ Case studies, whitepapers, webinars, videos\nAWS for Games Blog\n→ In-depth technical articles, customer success stories, performance optimization\nAWS Educate\n→ Free cloud courses for students, game-focused learning paths, community connections\nConclusion The future of gaming lies in the cloud.\nWith AWS Training and Certification, you can build the skills needed to design, develop, and scale modern games on the cloud.\nStart your learning journey today and join the community of cloud-savvy game developers building the next generation of games on AWS.\n"},{"uri":"https://hviethub.github.io/Internship-Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Apex Legends Migrates to Amazon GameLift in Just 10 Days Respawn Entertainment migrated Apex Legends to Amazon GameLift in just 10 days with no visible downtime. The move aimed to modernize hosting, scale globally, and maintain development speed.By partnering with AWS and Code Wizards Group, Respawn implemented a redirect layer, a game server wrapper, and real-time observability tools to ensure smooth matchmaking, zero-downtime updates, and better server monitoring.The migration was executed region by region, monitored continuously, and completed successfully, giving players improved performance and Respawn more control over infrastructure — all without disrupting gameplay or slowing development.\nBlog 2 - 6 Lessons Game Developers Can Learn from Epic Games’ Cloud Governance Strategy Epic Games partnered with AWS to design a cloud governance strategy that ensures security, cost efficiency, and scalability across their global operations. By implementing clear policies, automating configurations, and enforcing real-time compliance, Epic achieved almost complete resource tagging compliance and reduced cloud costs by about 20%. This approach highlights the importance of governance frameworks for both large studios and indie developers, proving that proper cloud management can enhance innovation while maintaining strong security and financial control.\nBlog 3 - Press Start on AWS Training for Game Development The AWS Training for Game Development guide helps developers build both game and cloud skills. It introduces key learning paths, including online courses on AWS Skill Builder, hands-on workshops, and industry-recognized AWS Certifications. Developers can also access GitHub repositories, technical articles, and educational programs like AWS Educate. By leveraging these resources, game creators can design scalable, low-latency architectures on AWS to support global players. The blog emphasizes that the future of gaming is on the cloud, and AWS Training \u0026amp; Certification equips developers with the tools and knowledge to succeed.\n"},{"uri":"https://hviethub.github.io/Internship-Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"AI-Driven Development Life Cycle Event Information Date: Friday, October 3, 2025\nTime: 2:00 PM – 4:30 PM\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nSpeakers: Toan Huynh, My Nguyen\nCoordinators: Diem My, Dai Truong, Dinh Nguyen\nRole: Attendee\nEvent Overview Workshop exploring how generative AI transforms the software development lifecycle through Amazon Q Developer and Kiro AI assistant.\nKey Topics Amazon Q Developer (2:15 – 3:30 PM) Speaker: Toan Huynh\nAI-Driven Development Lifecycle:\nPlanning \u0026amp; Architecture: AI-assisted design Development: Code generation from natural language Testing: Automated test generation Deployment: IaC generation Maintenance: Documentation and modernization Key Capabilities:\nCode generation with context awareness Code explanation and documentation Debugging assistance AWS integration (Lambda, CloudFormation, CDK) Live Demos:\nREST API endpoint generation Algorithm explanation Unit test creation Code refactoring AWS infrastructure code Kiro AI Assistant (3:45 – 4:30 PM) Speaker: My Nguyen\nCore Features:\nIntelligent code completion Multi-file understanding Project-level intelligence Automated refactoring Documentation generation Productivity Gains:\n40% faster code writing 60% reduction in boilerplate 50% less time on documentation 30% fewer initial bugs Live Demos:\nSmart code completion Multi-file refactoring Bug detection and fixing Test generation Documentation automation Key Takeaways AI-Driven Development:\nAI transforms every phase of development lifecycle Automation frees developers for creative work Productivity gains are substantial and measurable Quality improves through consistent patterns Amazon Q Developer:\nPowerful for AWS-centric development Natural language to code is highly effective Security and best practices built-in Multi-language support Kiro:\nExcels at project-level understanding Context awareness enhances suggestions Multi-file operations save significant time Customizable to team standards Best Practices:\nProvide clear context in prompts Review and understand generated code Iterate and refine suggestions Maintain security awareness Use for learning and skill development Next Steps:\nInstall Amazon Q Developer and Kiro Integrate into daily workflow Measure productivity improvements Share learnings with team Event Photos Rating: ⭐⭐⭐⭐⭐ (5/5)\nEye-opening demonstration of how AI tools are revolutionizing software development with practical productivity gains.\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.4-configure-stacks/","title":"Configure Infrastructure Stacks","tags":[],"description":"","content":"Overview Trong bước này, bạn sẽ cấu hình các CDK stacks cho infrastructure của EveryoneCook. Chúng ta sẽ cấu hình 7 stacks theo thứ tự dependency.\nProject Structure everyonecook-dev/ ├── infrastructure/ │ ├── bin/ │ │ └── app.ts # CDK app entry point │ ├── lib/ │ │ ├── base-stack.ts # Base stack class │ │ └── stacks/ │ │ ├── dns-stack.ts │ │ ├── certificate-stack.ts │ │ ├── core-stack.ts │ │ ├── auth-stack.ts │ │ ├── backend-stack.ts │ │ ├── frontend-stack.ts │ │ └── observability-stack.ts │ ├── config/ │ │ └── environment.ts # Environment configuration │ ├── cdk.json │ ├── package.json │ └── tsconfig.json ├── services/ # Lambda functions ├── shared/ # Shared code └── .env # Environment variables Step 1: Configure Environment Variables 1. Copy Example File\n# Copy .env.example to .env cp .env.example .env 2. Edit .env File\n# Open .env in your editor code .env # or vim .env, nano .env 3. Update Key Variables\n# AWS Configuration AWS_REGION=us-east-1 AWS_ACCOUNT_ID=123456789012 # Your AWS account ID AWS_PROFILE=default # Environment ENVIRONMENT=dev NODE_ENV=development # Domain Configuration DOMAIN_NAME=everyonecook.cloud API_DOMAIN=api.everyonecook.cloud CDN_DOMAIN=cdn.everyonecook.cloud FRONTEND_URL=https://everyonecook.cloud API_URL=https://api.everyonecook.cloud # OpenSearch (optional for dev) ENABLE_OPENSEARCH=false # Set to true if you want OpenSearch in dev # GitLab Configuration GITLAB_REPO=your-username/everyonecook GITLAB_BRANCH=main GITLAB_TOKEN=your-gitlab-token # Email Configuration SES_FROM_EMAIL=noreply@everyonecook.cloud ALERT_EMAIL=your-email@example.com Screenshot: .env file with configuration\nStep 2: Review Stack Configuration 1. Check Environment Config\n# Navigate to infrastructure directory cd infrastructure # View environment configuration cat config/environment.ts Cấu hình này định nghĩa:\nDomain names cho từng environment (dev, staging, prod) AWS account và region Feature flags (OpenSearch, etc.) Contact information 2. Review CDK App Entry Point\n# View CDK app configuration cat bin/app.ts File này:\nImport tất cả các stacks Tạo stack instances với dependencies Apply tags cho tất cả resources Step 3: Understand Stack Dependencies Các stacks phải deploy theo thứ tự vì có dependencies:\n1. DNS Stack (Route 53) ↓ 2. Certificate Stack (ACM - depends on DNS) ↓ 3. Core Stack (DynamoDB, S3, CloudFront, OpenSearch - depends on Certificate) ↓ 4. Auth Stack (Cognito, SES - depends on Core) ↓ 5. Backend Stack (API Gateway, Lambda - depends on Auth) ↓ 6. Frontend Stack (Amplify - depends on Backend) [Optional] ↓ 7. Observability Stack (CloudWatch, X-Ray - depends on all) Step 4: Configure Each Stack 1. DNS Stack Configuration\n// infrastructure/lib/stacks/dns-stack.ts // Tạo Route 53 Hosted Zone cho domain Key Configuration:\nDomain name: everyonecook.cloud Hosted zone type: Public Export nameservers để cấu hình ở domain registrar 2. Certificate Stack Configuration\n// infrastructure/lib/stacks/certificate-stack.ts // Tạo ACM certificates cho CloudFront và API Gateway Key Configuration:\nCloudFront certificate: cdn.everyonecook.cloud API Gateway certificate: *.everyonecook.cloud (wildcard) Region: us-east-1 (required for CloudFront) Validation: DNS via Route 53 3. Core Stack Configuration\n// infrastructure/lib/stacks/core-stack.ts // DynamoDB, S3, CloudFront, KMS, OpenSearch Key Configuration:\nDynamoDB: Single Table Design, username-based PK S3: 4 buckets (Content, Logs, Emails, CDN Logs) CloudFront: OAC, Shield Standard OpenSearch: Conditional (enabled if ENABLE_OPENSEARCH=true) KMS: 2 keys (DynamoDB, S3) 4. Auth Stack Configuration\n// infrastructure/lib/stacks/auth-stack.ts // Cognito, SES, Lambda triggers Key Configuration:\nCognito: Username/email sign-in, NO MFA Lambda triggers: 5 triggers (Pre-signup, Post-confirmation, etc.) SES: Email identity với DKIM Password policy: Min 12 chars 5. Backend Stack Configuration\n// infrastructure/lib/stacks/backend-stack.ts // API Gateway, Lambda, SQS, WAF Key Configuration:\nAPI Gateway: REST API với Cognito authorizer Lambda: 6 modules + 1 worker SQS: 6 queues + 6 DLQs WAF: API Gateway only (CloudFront uses Shield Standard) 6. Frontend Stack Configuration (Optional)\n// infrastructure/lib/stacks/frontend-stack.ts // AWS Amplify hosting Key Configuration:\nAmplify: Next.js 15 app GitLab integration Custom domain Auto-deploy on push 7. Observability Stack Configuration\n// infrastructure/lib/stacks/observability-stack.ts // CloudWatch, X-Ray Key Configuration:\nDashboards: 4 dashboards (Core, Auth, Backend, Overview) Alarms: Critical + Warning SNS: Email notifications X-Ray: Distributed tracing Step 5: Validate Configuration 1. Check TypeScript Compilation\n# Navigate to infrastructure directory cd infrastructure # Compile TypeScript npm run build # Should complete without errors Screenshot: Terminal showing successful TypeScript compilation\n2. Validate CDK Syntax\n# List all stacks npx cdk list --context environment=dev # Should show: # EveryoneCook-dev-DNS # EveryoneCook-dev-Certificate # EveryoneCook-dev-Core # EveryoneCook-dev-Auth # EveryoneCook-dev-Backend # EveryoneCook-dev-Observability Screenshot: Terminal showing all CDK stacks listed\n3. Synthesize CloudFormation Templates\n# Synthesize all stacks (generate CloudFormation templates) npx cdk synth --context environment=dev # This creates cdk.out/ directory with CloudFormation templates 4. Check Generated Templates\n# List generated templates ls -la cdk.out/ # Should show: # - EveryoneCook-dev-DNS.template.json # - EveryoneCook-dev-Certificate.template.json # - EveryoneCook-dev-Core.template.json # - EveryoneCook-dev-Auth.template.json # - EveryoneCook-dev-Backend.template.json # - EveryoneCook-dev-Observability.template.json Step 6: Review Stack Resources 1. DNS Stack Resources\n# View DNS stack template cat cdk.out/EveryoneCook-dev-DNS.template.json | jq \u0026#39;.Resources | keys\u0026#39; Resources:\nRoute 53 Hosted Zone NS records SOA record 2. Certificate Stack Resources\n# View Certificate stack template cat cdk.out/EveryoneCook-dev-Certificate.template.json | jq \u0026#39;.Resources | keys\u0026#39; Resources:\n2 ACM Certificates (CloudFront + API Gateway) DNS validation records 3. Core Stack Resources\n# View Core stack template cat cdk.out/EveryoneCook-dev-Core.template.json | jq \u0026#39;.Resources | keys\u0026#39; Resources:\nDynamoDB table + 5 GSI indexes 4 S3 buckets CloudFront distribution 2 KMS keys OpenSearch domain (if enabled) 4. Auth Stack Resources\n# View Auth stack template cat cdk.out/EveryoneCook-dev-Auth.template.json | jq \u0026#39;.Resources | keys\u0026#39; Resources:\nCognito User Pool 5 Lambda triggers SES email identity IAM roles 5. Backend Stack Resources\n# View Backend stack template cat cdk.out/EveryoneCook-dev-Backend.template.json | jq \u0026#39;.Resources | keys\u0026#39; Resources:\nAPI Gateway REST API 6 Lambda functions + 1 worker 6 SQS queues + 6 DLQs WAF Web ACL IAM roles 6. Observability Stack Resources\n# View Observability stack template cat cdk.out/EveryoneCook-dev-Observability.template.json | jq \u0026#39;.Resources | keys\u0026#39; Resources:\n4 CloudWatch dashboards CloudWatch alarms SNS topics Composite alarm Step 7: Estimate Costs Development Environment (without OpenSearch):\nDynamoDB: $3-5/month S3: $1-3/month CloudFront: $2-5/month Lambda: $0-3/month API Gateway: $0-3/month Cognito: $0/month (free tier) SES: $0-1/month Route 53: $0.50/month CloudWatch: $2-5/month WAF: $5-8/month Total: $15-35/month\nWith OpenSearch:\nAdd $50-100/month for OpenSearch domain Total with OpenSearch: $65-135/month\nTroubleshooting Issue: TypeScript compilation errors\n# Clean and rebuild cd infrastructure rm -rf node_modules dist npm install npm run build Issue: CDK synth fails\n# Check for syntax errors npx cdk synth --context environment=dev --verbose # Check environment variables cat ../.env Issue: Missing dependencies\n# Install all dependencies cd infrastructure npm install # Install root dependencies cd .. npm install Issue: Wrong AWS account\n# Verify AWS account aws sts get-caller-identity # Should match AWS_ACCOUNT_ID in .env Configuration Checklist .env file created and configured AWS_ACCOUNT_ID matches your account DOMAIN_NAME set correctly ENABLE_OPENSEARCH set (true/false) TypeScript compiles without errors CDK list shows all 6-7 stacks CDK synth generates templates No syntax errors in stack code Next Steps Once configuration is complete and validated, proceed to Deploy Infrastructure to deploy all stacks to AWS.\n"},{"uri":"https://hviethub.github.io/Internship-Report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Data Science On AWS Event Information Date: Thursday, October 16, 2025\nTime: 9:30 AM – 11:45 AM\nLocation: Hall Academic – FPT University\nSpeakers: Van Hoang Kha, Bach Doan Vuong\nRole: Attendee\nEvent Overview Workshop exploring how to build modern Data Science systems on AWS, from data processing to ML model deployment.\nKey Topics Introduction \u0026amp; AWS Data Science Pipeline (9:30 – 10:05 AM)\nWhy Cloud for Data Science:\nOptimize performance and reduce costs Scale flexibly with demand Access to latest hardware (GPUs, TPUs) Pay-as-you-go pricing model AWS Data Science Ecosystem:\nAmazon S3: Scalable data lake storage AWS Glue: Serverless ETL service Amazon SageMaker: End-to-end ML platform Demo 1: Data Processing with AWS Glue (10:05 – 10:35 AM)\nIMDb Dataset Processing:\nData discovery with Glue Crawler Data cleaning and transformation Text preprocessing and feature extraction Format conversion (CSV to Parquet) PySpark for distributed processing Key Benefits:\nServerless, auto-scaling Pay only for job execution time Built-in data catalog Integration with AWS services Demo 2: Sentiment Analysis with SageMaker (10:35 – 11:00 AM)\nML Workflow:\nData preparation and EDA Model selection and training Hyperparameter tuning Model deployment as endpoint Real-time inference testing Results:\n90%+ accuracy on test set Low latency predictions Auto-scaling for production Cost-effective deployment Cloud vs On-Premise Discussion (11:00 – 11:35 AM)\nCost Comparison:\nSmall projects: Cloud 60-70% cheaper No upfront hardware investment Pay-as-you-go reduces waste Performance:\nAccess to latest hardware Distributed computing capabilities Global infrastructure Flexibility:\nScale up/down in minutes No capacity planning needed Easy experimentation Post-Workshop Project (11:35 – 11:45 AM)\nBuild complete Data Science pipeline Suggested projects: Product review analyzer, stock predictor, churn prediction 4-week implementation timeline Key Takeaways AWS Data Science Services:\nS3 provides scalable, durable storage for data lakes Glue enables serverless ETL and data cataloging SageMaker offers end-to-end ML capabilities Managed services reduce operational complexity Practical Skills:\nData cleaning and transformation with Glue Model training and deployment with SageMaker Cost optimization strategies Cloud vs on-premise decision framework Cloud Benefits:\nFaster time-to-market Lower costs for most use cases Superior scalability and flexibility Access to latest technologies Next Steps:\nCreate AWS Free Tier account Complete post-workshop project Experiment with Glue and SageMaker Pursue AWS ML certification Event Photos Rating: ⭐⭐⭐⭐⭐ (5/5)\nExcellent hands-on workshop demonstrating how cloud computing transforms Data Science workflows with practical AWS services.\n"},{"uri":"https://hviethub.github.io/Internship-Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 AM – 12:00 PM, Saturday, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30 AM – 12:00 PM, Monday, November 17, 2025\nLocation: Bitexco Financial Tower, HCMC\nRole: Attendee\nEvent 3 Event Name: AI-Driven Development Life Cycle\nDate \u0026amp; Time: 2:00 PM – 4:30 PM, Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Attendee\nEvent 4 Event Name: Data Science On AWS\nDate \u0026amp; Time: 9:30 AM – 11:45 AM, Thursday, October 16, 2025\nLocation: Hall Academic – FPT University\nRole: Attendee\nEvent 5 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 8:30 AM – 12:00 PM, Friday, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.5-deploy-infrastructure/","title":"Deploy Infrastructure","tags":[],"description":"","content":"Overview Trong bước này, bạn sẽ deploy tất cả infrastructure stacks lên AWS theo đúng thứ tự dependencies. Quá trình này mất khoảng 30-45 phút.\nDeployment Order 1. DNS Stack (2-3 phút) 2. Certificate Stack (5-10 phút - DNS validation) 3. Core Stack (10-15 phút - CloudFront + OpenSearch) 4. Auth Stack (5-7 phút - Cognito + SES) 5. Backend Stack (8-12 phút - API Gateway + Lambda) 6. Observability Stack (3-5 phút - CloudWatch) Total time: 30-50 phút\nStep 1: Deploy DNS Stack 1. Deploy Stack\n# Navigate to infrastructure directory cd infrastructure # Deploy DNS stack npx cdk deploy EveryoneCook-dev-DNS --context environment=dev # Review changes and type \u0026#39;y\u0026#39; to confirm Screenshot: Terminal showing DNS stack deployment\n2. Get Nameservers\nSau khi deploy xong, lưu lại 4 nameservers từ output:\nOutputs: EveryoneCook-dev-DNS.NameServers = ns-123.awsdns-12.com, ns-456.awsdns-45.net, ... EveryoneCook-dev-DNS.HostedZoneId = Z1234567890ABC 3. Update Domain Registrar\nĐi đến domain registrar (Hostinger, GoDaddy, etc.) và update nameservers:\nLogin vào domain registrar Tìm domain của bạn (everyonecook.cloud) Chọn \u0026ldquo;Custom Nameservers\u0026rdquo; Nhập 4 nameservers từ AWS Save changes Screenshot: Hostinger showing nameservers updated\n4. Wait for DNS Propagation\n# Check DNS propagation (có thể mất 5-30 phút) dig NS everyonecook.cloud # Hoặc dùng online tool: https://www.whatsmydns.net/ Step 2: Deploy Certificate Stack Important: Stack này phải deploy ở us-east-1 region (CloudFront requirement)\n1. Deploy Stack\n# Deploy Certificate stack npx cdk deploy EveryoneCook-dev-Certificate --context environment=dev # Type \u0026#39;y\u0026#39; to confirm 2. Wait for DNS Validation\nACM sẽ tự động:\nTạo CNAME records trong Route 53 Validate domain ownership Issue certificates Quá trình này mất 5-10 phút.\nScreenshot: ACM showing certificates being validated\n3. Verify Certificates\n# Check certificate status aws acm list-certificates --region us-east-1 # Both certificates should show Status: ISSUED Screenshot: ACM console showing both certificates issued\nStep 3: Deploy Core Stack 1. Deploy Stack\n# Deploy Core stack (takes 10-15 minutes) npx cdk deploy EveryoneCook-dev-Core --context environment=dev # Type \u0026#39;y\u0026#39; to confirm Deployment includes:\nDynamoDB table creation 4 S3 buckets CloudFront distribution (takes longest) 2 KMS keys OpenSearch domain (if enabled) Screenshot: Terminal showing Core stack deployment progress\n2. Monitor Deployment\n# In another terminal, watch CloudFormation events aws cloudformation describe-stack-events \\ --stack-name EveryoneCook-dev-Core \\ --max-items 10 \\ --query \u0026#39;StackEvents[*].[Timestamp,ResourceStatus,ResourceType,LogicalResourceId]\u0026#39; \\ --output table 3. Verify Resources\n# Check DynamoDB table aws dynamodb describe-table --table-name EveryoneCook-dev # Check S3 buckets aws s3 ls | grep everyonecook # Check CloudFront distribution aws cloudfront list-distributions --query \u0026#39;DistributionList.Items[*].[Id,DomainName,Status]\u0026#39; Screenshot: AWS Console showing Core stack resources\nStep 4: Deploy Auth Stack 1. Deploy Stack\n# Deploy Auth stack npx cdk deploy EveryoneCook-dev-Auth --context environment=dev # Type \u0026#39;y\u0026#39; to confirm Deployment includes:\nCognito User Pool 5 Lambda triggers SES email identity IAM roles 2. Verify SES Email Identity\n# Check SES identity status aws sesv2 get-email-identity --email-identity everyonecook.cloud # Should show: VerificationStatus: SUCCESS 3. Request SES Production Access\nNếu muốn gửi email thực:\nGo to SES Console → Account dashboard Click \u0026ldquo;Request production access\u0026rdquo; Fill form: Use case: Transactional emails Website: https://everyonecook.cloud Describe bounce handling Submit (usually approved in 24 hours) Screenshot: SES console showing email identity verified\nStep 5: Deploy Backend Stack 1. Prepare Backend Code\n# Navigate to project root cd .. # Build and prepare all Lambda modules .\\prepare-backend-deployment.ps1 # This script: # - Compiles TypeScript to JavaScript # - Installs production dependencies # - Creates deployment packages Screenshot: Terminal showing backend preparation\n2. Deploy Stack\n# Navigate back to infrastructure cd infrastructure # Deploy Backend stack npx cdk deploy EveryoneCook-dev-Backend --context environment=dev # Type \u0026#39;y\u0026#39; to confirm Deployment includes:\nAPI Gateway REST API 6 Lambda modules + 1 worker 6 SQS queues + 6 DLQs WAF Web ACL 3. Verify API Gateway\n# Get API endpoint aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Backend \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue\u0026#39; \\ --output text # Test health endpoint curl https://api.everyonecook.cloud/health Screenshot: API Gateway console showing REST API deployed\nStep 6: Deploy Observability Stack 1. Deploy Stack\n# Deploy Observability stack npx cdk deploy EveryoneCook-dev-Observability --context environment=dev # Type \u0026#39;y\u0026#39; to confirm Deployment includes:\n4 CloudWatch dashboards CloudWatch alarms SNS topics Composite alarm 2. Subscribe to SNS Topic\n# Get SNS topic ARN TOPIC_ARN=$(aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Observability \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`AlarmTopicArn`].OutputValue\u0026#39; \\ --output text) # Subscribe to email notifications aws sns subscribe \\ --topic-arn $TOPIC_ARN \\ --protocol email \\ --notification-endpoint your-email@example.com # Check email and confirm subscription Screenshot: CloudWatch console showing dashboards created\nStep 7: Verify All Stacks 1. List All Stacks\n# List all deployed stacks aws cloudformation list-stacks \\ --stack-status-filter CREATE_COMPLETE \\ --query \u0026#39;StackSummaries[?contains(StackName, `EveryoneCook-dev`)].StackName\u0026#39; Should show:\nEveryoneCook-dev-DNS EveryoneCook-dev-Certificate EveryoneCook-dev-Core EveryoneCook-dev-Auth EveryoneCook-dev-Backend EveryoneCook-dev-Observability 2. Check Stack Outputs\n# Get all stack outputs for stack in DNS Certificate Core Auth Backend Observability; do echo \u0026#34;=== EveryoneCook-dev-$stack ===\u0026#34; aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-$stack \\ --query \u0026#39;Stacks[0].Outputs[*].[OutputKey,OutputValue]\u0026#39; \\ --output table done 3. Verify Resources\n# DynamoDB aws dynamodb list-tables | grep EveryoneCook # S3 aws s3 ls | grep everyonecook # Lambda aws lambda list-functions | grep EveryoneCook # API Gateway aws apigateway get-rest-apis | grep EveryoneCook # Cognito aws cognito-idp list-user-pools --max-results 10 | grep EveryoneCook Screenshot: CloudFormation console showing all stacks deployed\nDeployment Summary Deployed Resources:\n✅ Route 53 Hosted Zone ✅ 2 ACM Certificates (CloudFront + API Gateway) ✅ DynamoDB Table with 5 GSI indexes ✅ 4 S3 Buckets ✅ CloudFront Distribution ✅ 2 KMS Keys ✅ OpenSearch Domain (if enabled) ✅ Cognito User Pool + 5 Lambda Triggers ✅ SES Email Identity ✅ API Gateway REST API ✅ 7 Lambda Functions (6 modules + 1 worker) ✅ 12 SQS Queues (6 main + 6 DLQ) ✅ WAF Web ACL ✅ 4 CloudWatch Dashboards ✅ CloudWatch Alarms + SNS Topics Total Resources: ~100+ AWS resources\nTroubleshooting Issue: Stack deployment fails\n# Check CloudFormation events aws cloudformation describe-stack-events \\ --stack-name EveryoneCook-dev-STACKNAME \\ --max-items 20 # Look for CREATE_FAILED or ROLLBACK events Issue: Certificate validation stuck\n# Check DNS propagation dig everyonecook.cloud # Check validation records in Route 53 aws route53 list-resource-record-sets \\ --hosted-zone-id YOUR-ZONE-ID \\ --query \u0026#39;ResourceRecordSets[?Type==`CNAME`]\u0026#39; Issue: Lambda deployment fails\n# Ensure backend code is prepared cd .. .\\prepare-backend-deployment.ps1 # Check Lambda packages exist ls services/*/deployment/ Issue: Insufficient permissions\n# Check your IAM permissions aws iam get-user # Ensure you have AdministratorAccess or equivalent Cost Tracking After Deployment:\n# Check estimated costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=SERVICE Expected Costs (Dev):\nFirst month: $20-35 (without OpenSearch) With OpenSearch: $70-135 Most costs: DynamoDB, S3, CloudFront, WAF Next Steps Once all infrastructure is deployed, proceed to Configure API \u0026amp; Lambda to set up your API routes and Lambda functions.\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building EveryoneCook: A Full-Stack AWS Infrastructure Workshop Overview In this comprehensive workshop, you will learn how to build a production-ready, full-stack social cooking application infrastructure on AWS using Infrastructure as Code (IaC) with AWS CDK. The EveryoneCook platform demonstrates modern cloud architecture patterns, including serverless computing, content delivery, AI-powered features, advanced search with OpenSearch, and comprehensive observability.\nYou will deploy seven interconnected CDK stacks that work together to create a scalable, secure, and cost-optimized application:\nDNS Stack - Route 53 hosted zone for domain management Certificate Stack - ACM certificates for CloudFront and API Gateway (us-east-1) CoreStack - DynamoDB Single Table, S3 buckets, CloudFront CDN, KMS encryption, and OpenSearch AuthStack - Cognito User Pool with Lambda triggers and SES email integration BackendStack - API Gateway, Lambda functions (6 modules), SQS queues, and WAF protection FrontendStack - AWS Amplify hosting for Next.js 15 application (optional) ObservabilityStack - CloudWatch dashboards, alarms, and X-Ray distributed tracing Content Workshop Overview Setup Environment CDK Bootstrap Configure Infrastructure Stacks Deploy Infrastructure Configure API \u0026amp; Lambda Deploy Backend Services Test Endpoints End-to-End Push to GitLab Deploy to Amplify Clean up "},{"uri":"https://hviethub.github.io/Internship-Report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"AWS Well-Architected Security Pillar Workshop Event Information Date: Friday, November 29, 2025\nTime: 8:30 AM – 12:00 PM (Morning Only)\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu St., District 1, HCMC\nSpeakers: Tran Toan Cong Ly, Hoang Anh, Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat, Hoang Kha, Thinh Lam, Viet Nguyen, Mende Grabski (Long), Tinh Truong\nRole: Attendee\nEvent Overview Comprehensive workshop on AWS Well-Architected Security Pillar, covering the five fundamental pillars of cloud security with practical demonstrations and real-world scenarios from Vietnamese enterprises.\nSession Schedule 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Speakers: Tran Toan Cong Ly\nSecurity Pillar Role in Well-Architected Framework: Core principles: Least Privilege, Zero Trust, Defense in Depth AWS Shared Responsibility Model Top security threats in Vietnam\u0026rsquo;s cloud environment ⭐ Pillar 1 — Identity \u0026amp; Access Management\n8:50 – 9:30 AM | Modern IAM Architecture Speakers: Hoang Anh\nIAM Fundamentals:\nUsers, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO and permission sets Service Control Policies (SCP) \u0026amp; permission boundaries for multi-account MFA, credential rotation, Access Analyzer Mini Demo:\nValidate IAM Policy Simulate access scenarios ⭐ Pillar 2 — Detection\n9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring Speakers: Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat\nMonitoring Services:\nCloudTrail (organization-level logging) GuardDuty (threat detection) Security Hub (centralized security view) Comprehensive Logging:\nVPC Flow Logs ALB/S3 access logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules) 9:55 – 10:10 AM | Coffee Break\n⭐ Pillar 3 — Infrastructure Protection\n10:10 – 10:40 AM | Network \u0026amp; Workload Security Speakers: Hoang Kha\nNetwork Security:\nVPC segmentation strategies Private vs public subnet placement Security Groups vs NACLs: application models Protection Services:\nAWS WAF (Web Application Firewall) AWS Shield (DDoS protection) AWS Network Firewall Workload Protection:\nEC2 security basics ECS/EKS container security ⭐ Pillar 4 — Data Protection\n10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets Speakers: Thinh Lam, Viet Nguyen\nKey Management:\nAWS KMS: key policies, grants, rotation Encryption at-rest: S3, EBS, RDS, DynamoDB Encryption in-transit: TLS/SSL Secrets Management:\nAWS Secrets Manager Systems Manager Parameter Store Rotation patterns and best practices Data Governance:\nData classification strategies Access guardrails and policies ⭐ Pillar 5 — Incident Response\n11:10 – 11:40 AM | IR Playbook \u0026amp; Automation Speakers: Mende Grabski (Long), Tinh Truong\nIncident Response Lifecycle:\nPreparation Detection \u0026amp; Analysis Containment Eradication \u0026amp; Recovery Post-Incident Activity IR Playbooks:\nCompromised IAM key response S3 public exposure remediation EC2 malware detection handling Automation:\nSnapshot and evidence collection Instance isolation procedures Auto-response with Lambda/Step Functions 11:40 – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A\nSummary of 5 Security Pillars\nCommon Pitfalls:\nReal-world challenges in Vietnamese enterprises Lessons learned from security incidents Security Learning Roadmap:\nAWS Certified Security – Specialty AWS Certified Solutions Architect – Professional Continuous learning resources Key Takeaways Security Foundation:\nSecurity is a shared responsibility between AWS and customers Implement defense in depth with multiple security layers Zero Trust approach: verify explicitly, use least privilege, assume breach IAM Best Practices:\nEliminate long-term credentials, use temporary credentials Implement MFA for all human users Use IAM roles for applications and services Regular access reviews with Access Analyzer Detection \u0026amp; Monitoring:\nEnable CloudTrail at organization level Implement GuardDuty for threat detection Centralize security findings with Security Hub Automate responses to security events Infrastructure Protection:\nDesign VPC with proper segmentation Use Security Groups as primary firewall Implement WAF for web applications Protect workloads with appropriate security controls Data Protection:\nEncrypt data at rest and in transit Use KMS for centralized key management Rotate secrets regularly with Secrets Manager Classify data and apply appropriate controls Incident Response:\nPrepare IR playbooks before incidents occur Automate response actions where possible Practice incident response procedures Learn from incidents and improve Vietnamese Enterprise Context:\nCommon security challenges in local market Compliance requirements (PDPA, local regulations) Cost-effective security implementation strategies Event Photos Rating: ⭐⭐⭐⭐⭐ (5/5)\nComprehensive and practical security workshop covering all essential aspects of AWS security with real-world scenarios relevant to Vietnamese enterprises.\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.6-configure-api-lambda/","title":"Configure API &amp; Lambda","tags":[],"description":"","content":"Overview Sau khi deploy infrastructure, bạn cần cấu hình API Gateway routes và Lambda functions để xử lý business logic.\nAPI Architecture API Gateway (api.everyonecook.cloud) ↓ API Router Lambda (entry point) ↓ ┌─────────┬─────────┬─────────┬─────────┬─────────┐ │ Auth │ Social │ Recipe │ Admin │ Upload │ │ Module │ Module │ AI │ Module │ Module │ └─────────┴─────────┴─────────┴─────────┴─────────┘ ↓ ↓ ↓ ↓ ↓ DynamoDB DynamoDB Bedrock DynamoDB S3 Step 1: Review API Routes API Router Structure:\n// services/api-router/routes/index.ts export const routes = { // Auth routes \u0026#39;POST /auth/register\u0026#39;: \u0026#39;auth-module\u0026#39;, \u0026#39;POST /auth/login\u0026#39;: \u0026#39;auth-module\u0026#39;, \u0026#39;GET /auth/profile\u0026#39;: \u0026#39;auth-module\u0026#39;, \u0026#39;PUT /auth/profile\u0026#39;: \u0026#39;auth-module\u0026#39;, // Social routes \u0026#39;GET /social/posts\u0026#39;: \u0026#39;social-module\u0026#39;, \u0026#39;POST /social/posts\u0026#39;: \u0026#39;social-module\u0026#39;, \u0026#39;GET /social/posts/:id\u0026#39;: \u0026#39;social-module\u0026#39;, \u0026#39;POST /social/posts/:id/like\u0026#39;: \u0026#39;social-module\u0026#39;, // Recipe routes \u0026#39;GET /recipes\u0026#39;: \u0026#39;recipe-module\u0026#39;, \u0026#39;POST /recipes\u0026#39;: \u0026#39;recipe-module\u0026#39;, \u0026#39;POST /ai/generate-recipe\u0026#39;: \u0026#39;recipe-module\u0026#39;, \u0026#39;POST /ai/search\u0026#39;: \u0026#39;recipe-module\u0026#39;, // Admin routes \u0026#39;GET /admin/users\u0026#39;: \u0026#39;admin-module\u0026#39;, \u0026#39;POST /admin/users/:id/ban\u0026#39;: \u0026#39;admin-module\u0026#39;, // Upload routes \u0026#39;POST /upload/presigned-url\u0026#39;: \u0026#39;upload-module\u0026#39;, \u0026#39;POST /upload/complete\u0026#39;: \u0026#39;upload-module\u0026#39; }; Step 2: Configure Lambda Modules 1. Auth Module\ncd services/auth-module # Review configuration cat package.json cat tsconfig.json # Check environment variables needed cat index.ts | grep process.env Environment Variables:\nTABLE_NAME: DynamoDB table name USER_POOL_ID: Cognito User Pool ID REGION: AWS region 2. Social Module\ncd services/social-module # Review handlers ls handlers/ # - posts.handler.ts # - comments.handler.ts # - reactions.handler.ts # - friends.handler.ts 3. Recipe/AI Module\ncd services/recipe-module # Check AI configuration cat services/ai.service.ts Environment Variables:\nBEDROCK_MODEL_ID: Claude 3.5 Sonnet v2 BEDROCK_REGION: us-east-1 OPENSEARCH_ENDPOINT: OpenSearch domain (if enabled) 4. Admin Module\ncd services/admin-module # Review admin operations ls handlers/ 5. Upload Module\ncd services/upload-module # Check S3 configuration cat services/s3.service.ts Environment Variables:\nCONTENT_BUCKET: S3 content bucket name CLOUDFRONT_DOMAIN: CloudFront domain CLOUDFRONT_KEY_PAIR_ID: For signed URLs Step 3: Configure Lambda Layers Shared Dependencies:\ncd services/shared # Review shared code ls -la # - repositories/ # - business-logic/ # - utils/ # - types/ Lambda Layer Structure:\nshared/ ├── repositories/ │ ├── user.repository.ts │ ├── post.repository.ts │ └── recipe.repository.ts ├── business-logic/ │ ├── auth.logic.ts │ └── validation.logic.ts └── utils/ ├── dynamodb.utils.ts └── response.utils.ts Step 4: Set Lambda Environment Variables 1. Get Stack Outputs\n# Get DynamoDB table name TABLE_NAME=$(aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Core \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DynamoDBTableName`].OutputValue\u0026#39; \\ --output text) # Get User Pool ID USER_POOL_ID=$(aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Auth \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`UserPoolId`].OutputValue\u0026#39; \\ --output text) # Get Content Bucket CONTENT_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Core \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ContentBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;TABLE_NAME=$TABLE_NAME\u0026#34; echo \u0026#34;USER_POOL_ID=$USER_POOL_ID\u0026#34; echo \u0026#34;CONTENT_BUCKET=$CONTENT_BUCKET\u0026#34; 2. Update Lambda Environment Variables\nLambda environment variables được set tự động bởi CDK stack, nhưng bạn có thể verify:\n# Check Auth Module environment variables aws lambda get-function-configuration \\ --function-name EveryoneCook-dev-AuthModule \\ --query \u0026#39;Environment.Variables\u0026#39; # Should show: # { # \u0026#34;TABLE_NAME\u0026#34;: \u0026#34;EveryoneCook-dev\u0026#34;, # \u0026#34;USER_POOL_ID\u0026#34;: \u0026#34;us-east-1_ABC123\u0026#34;, # \u0026#34;REGION\u0026#34;: \u0026#34;us-east-1\u0026#34; # } Step 5: Configure API Gateway 1. Review API Gateway Configuration\n# Get API Gateway ID API_ID=$(aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Backend \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiId`].OutputValue\u0026#39; \\ --output text) # Get API details aws apigateway get-rest-api --rest-api-id $API_ID 2. Check Cognito Authorizer\n# List authorizers aws apigateway get-authorizers --rest-api-id $API_ID # Should show Cognito User Pool authorizer 3. Verify Custom Domain\n# Check custom domain mapping aws apigateway get-domain-name --domain-name api.everyonecook.cloud # Should show: # - Domain name: api.everyonecook.cloud # - Certificate ARN # - Regional domain name Step 6: Configure SQS Queues 1. List All Queues\n# List SQS queues aws sqs list-queues | grep EveryoneCook-dev # Should show 12 queues (6 main + 6 DLQ) 2. Configure Queue Permissions\nPermissions được set tự động bởi CDK, verify:\n# Get AI Queue URL AI_QUEUE_URL=$(aws sqs list-queues \\ --queue-name-prefix EveryoneCook-dev-AIQueue \\ | jq -r \u0026#39;.QueueUrls[0]\u0026#39;) # Check queue attributes aws sqs get-queue-attributes \\ --queue-url $AI_QUEUE_URL \\ --attribute-names All Step 7: Configure Lambda Triggers Cognito Lambda Triggers:\n# Check User Pool triggers aws cognito-idp describe-user-pool \\ --user-pool-id $USER_POOL_ID \\ --query \u0026#39;UserPool.LambdaConfig\u0026#39; # Should show 5 triggers: # - PreSignUp # - PostConfirmation # - PreAuthentication # - PostAuthentication # - CustomMessage Step 8: Test Lambda Functions Locally 1. Test Auth Module\ncd services/auth-module # Run tests npm test # Test specific handler npm run test:unit -- handlers/profile.test.ts 2. Test API Router\ncd services/api-router # Test routing logic npm test # Test with sample event node -e \u0026#34; const handler = require(\u0026#39;./dist/index\u0026#39;).handler; const event = { httpMethod: \u0026#39;GET\u0026#39;, path: \u0026#39;/health\u0026#39;, headers: {} }; handler(event).then(console.log); \u0026#34; Step 9: Verify IAM Permissions 1. Check Lambda Execution Roles\n# List Lambda functions aws lambda list-functions \\ --query \u0026#39;Functions[?contains(FunctionName, `EveryoneCook-dev`)].FunctionName\u0026#39; # Check role for Auth Module aws lambda get-function \\ --function-name EveryoneCook-dev-AuthModule \\ --query \u0026#39;Configuration.Role\u0026#39; 2. Verify DynamoDB Permissions\n# Get role name ROLE_NAME=$(aws lambda get-function \\ --function-name EveryoneCook-dev-AuthModule \\ --query \u0026#39;Configuration.Role\u0026#39; \\ --output text | cut -d\u0026#39;/\u0026#39; -f2) # List attached policies aws iam list-attached-role-policies --role-name $ROLE_NAME # Should include DynamoDB access policy Configuration Checklist API routes reviewed and understood Lambda modules structure reviewed Environment variables verified Lambda layers configured API Gateway custom domain working Cognito authorizer configured SQS queues created and accessible Lambda triggers attached to Cognito IAM permissions verified Local tests passing Next Steps Once configuration is complete, proceed to Deploy Backend Services to deploy your Lambda code.\n"},{"uri":"https://hviethub.github.io/Internship-Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from September 8, 2025 to November 12, 2025, I had the opportunity to learn, practice and apply the knowledge I had acquired at school to the actual working environment.I participated in learning about AWS cloud services and, together with team members, carried out a project applying that knowledge, thereby improving my skills in programming, analysis, report writing, communication, etc..\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.7-deploy-backend/","title":"Deploy Backend Services","tags":[],"description":"","content":"Overview Sau khi cấu hình xong, bạn sẽ deploy Lambda code lên AWS. Quá trình này bao gồm compile TypeScript, install dependencies, và update Lambda functions.\nDeployment Process 1. Compile TypeScript → JavaScript 2. Install production dependencies 3. Create deployment packages 4. Update Lambda functions 5. Verify deployments 6. Check CloudWatch logs Step 1: Prepare Backend Deployment 1. Run Preparation Script\n# Navigate to project root cd /path/to/everyonecook-dev # Run preparation script (PowerShell) .\\prepare-backend-deployment.ps1 Script Actions:\nCompiles TypeScript to JavaScript for all modules Installs production dependencies (no devDependencies) Creates deployment packages Ensures CloudFormation detects changes Screenshot: Terminal showing preparation script running\n2. Verify Preparation\n# Run verification script .\\verify-deployment.ps1 # Checks: # - All modules have dist/ folder # - All modules have node_modules/ # - All modules have package.json # - Deployment packages are ready Step 2: Deploy Lambda Functions Option A: Deploy All Functions (Recommended)\n# Navigate to infrastructure cd infrastructure # Deploy Backend stack (updates all Lambdas) npx cdk deploy EveryoneCook-dev-Backend --context environment=dev # Type \u0026#39;y\u0026#39; to confirm Option B: Deploy Individual Functions\n# Update specific Lambda function aws lambda update-function-code \\ --function-name EveryoneCook-dev-AuthModule \\ --zip-file fileb://services/auth-module/deployment/lambda.zip # Wait for update to complete aws lambda wait function-updated \\ --function-name EveryoneCook-dev-AuthModule Screenshot: Terminal showing Lambda functions being updated\nStep 3: Verify Lambda Deployments 1. Check Function Status\n# List all Lambda functions aws lambda list-functions \\ --query \u0026#39;Functions[?contains(FunctionName, `EveryoneCook-dev`)].{Name:FunctionName,Runtime:Runtime,Updated:LastModified}\u0026#39; \\ --output table 2. Check Function Configuration\n# Get Auth Module details aws lambda get-function \\ --function-name EveryoneCook-dev-AuthModule \\ --query \u0026#39;{Runtime:Configuration.Runtime,Handler:Configuration.Handler,Timeout:Configuration.Timeout,Memory:Configuration.MemorySize}\u0026#39; 3. Verify Code SHA256\n# Check code hash (should be different after update) aws lambda get-function \\ --function-name EveryoneCook-dev-AuthModule \\ --query \u0026#39;Configuration.CodeSha256\u0026#39; Step 4: Test Lambda Functions 1. Test API Router\n# Invoke API Router with test event aws lambda invoke \\ --function-name EveryoneCook-dev-APIRouter \\ --payload \u0026#39;{\u0026#34;httpMethod\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/health\u0026#34;,\u0026#34;headers\u0026#34;:{}}\u0026#39; \\ response.json # Check response cat response.json # Should return: {\u0026#34;statusCode\u0026#34;:200,\u0026#34;body\u0026#34;:\u0026#34;{\\\u0026#34;status\\\u0026#34;:\\\u0026#34;healthy\\\u0026#34;}\u0026#34;} 2. Test Auth Module\n# Test get profile (requires valid token) aws lambda invoke \\ --function-name EveryoneCook-dev-AuthModule \\ --payload \u0026#39;{ \u0026#34;httpMethod\u0026#34;:\u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/auth/profile\u0026#34;, \u0026#34;headers\u0026#34;:{\u0026#34;Authorization\u0026#34;:\u0026#34;Bearer test-token\u0026#34;}, \u0026#34;requestContext\u0026#34;:{\u0026#34;authorizer\u0026#34;:{\u0026#34;claims\u0026#34;:{\u0026#34;sub\u0026#34;:\u0026#34;test-user-id\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;testuser\u0026#34;}}} }\u0026#39; \\ response.json cat response.json 3. Test Social Module\n# Test get posts aws lambda invoke \\ --function-name EveryoneCook-dev-SocialModule \\ --payload \u0026#39;{ \u0026#34;httpMethod\u0026#34;:\u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/social/posts\u0026#34;, \u0026#34;headers\u0026#34;:{\u0026#34;Authorization\u0026#34;:\u0026#34;Bearer test-token\u0026#34;}, \u0026#34;requestContext\u0026#34;:{\u0026#34;authorizer\u0026#34;:{\u0026#34;claims\u0026#34;:{\u0026#34;sub\u0026#34;:\u0026#34;test-user-id\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;testuser\u0026#34;}}} }\u0026#39; \\ response.json cat response.json Step 5: Check CloudWatch Logs 1. View Recent Logs\n# Tail Auth Module logs aws logs tail /aws/lambda/EveryoneCook-dev-AuthModule --follow # Or view last 10 minutes aws logs tail /aws/lambda/EveryoneCook-dev-AuthModule --since 10m 2. Search for Errors\n# Search for errors in last hour aws logs filter-log-events \\ --log-group-name /aws/lambda/EveryoneCook-dev-AuthModule \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s)000 \\ --filter-pattern \u0026#34;ERROR\u0026#34; 3. Check Lambda Insights\n# Get Lambda metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=EveryoneCook-dev-AuthModule \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Sum Screenshot: CloudWatch Logs showing Lambda execution\nStep 6: Deploy Lambda Triggers Lambda triggers đã được deploy với Auth Stack, verify:\n# Check Post-Confirmation trigger aws lambda get-function \\ --function-name EveryoneCook-dev-PostConfirmationTrigger # Test trigger (will be invoked automatically on user confirmation) Step 7: Deploy Search Sync Worker 1. Deploy Worker\n# Worker được deploy với Backend Stack # Verify deployment aws lambda get-function \\ --function-name EveryoneCook-dev-SearchSyncWorker 2. Test Worker with SQS\n# Get SearchIndex queue URL QUEUE_URL=$(aws sqs list-queues \\ --queue-name-prefix EveryoneCook-dev-SearchIndexQueue \\ | jq -r \u0026#39;.QueueUrls[0]\u0026#39;) # Send test message aws sqs send-message \\ --queue-url $QUEUE_URL \\ --message-body \u0026#39;{ \u0026#34;eventName\u0026#34;: \u0026#34;INSERT\u0026#34;, \u0026#34;tableName\u0026#34;: \u0026#34;recipes\u0026#34;, \u0026#34;keys\u0026#34;: {\u0026#34;PK\u0026#34;: \u0026#34;USER#testuser\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;RECIPE#test-123\u0026#34;}, \u0026#34;newImage\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;Test Recipe\u0026#34;, \u0026#34;cuisine\u0026#34;: \u0026#34;Vietnamese\u0026#34;} }\u0026#39; # Check worker logs aws logs tail /aws/lambda/EveryoneCook-dev-SearchSyncWorker --follow Step 8: Update API Gateway API Gateway tự động update khi deploy Backend Stack, verify:\n# Get API Gateway deployment API_ID=$(aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Backend \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiId`].OutputValue\u0026#39; \\ --output text) # List deployments aws apigateway get-deployments --rest-api-id $API_ID # Get latest deployment aws apigateway get-deployment \\ --rest-api-id $API_ID \\ --deployment-id $(aws apigateway get-deployments \\ --rest-api-id $API_ID \\ --query \u0026#39;items[0].id\u0026#39; \\ --output text) Step 9: Warm Up Lambda Functions Tránh cold start cho requests đầu tiên:\n# Invoke all functions once for func in APIRouter AuthModule SocialModule RecipeAIModule AdminModule UploadModule SearchSyncWorker; do echo \u0026#34;Warming up $func...\u0026#34; aws lambda invoke \\ --function-name EveryoneCook-dev-$func \\ --payload \u0026#39;{\u0026#34;warmup\u0026#34;:true}\u0026#39; \\ /dev/null done Step 10: Verify End-to-End 1. Test Health Endpoint\n# Test via API Gateway curl https://api.everyonecook.cloud/health # Should return: {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;...\u0026#34;} 2. Test with Postman\nImport Postman collection:\n{ \u0026#34;info\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;EveryoneCook API\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;https://schema.getpostman.com/json/collection/v2.1.0/collection.json\u0026#34; }, \u0026#34;item\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Health Check\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.everyonecook.cloud/health\u0026#34; } } ] } Deployment Checklist Preparation script completed successfully All Lambda functions updated Function status: Active Test invocations successful CloudWatch logs showing executions No errors in logs Lambda triggers working Search Sync Worker processing messages API Gateway updated Health endpoint responding Functions warmed up Troubleshooting Issue: Lambda update fails\n# Check function state aws lambda get-function \\ --function-name EveryoneCook-dev-AuthModule \\ --query \u0026#39;Configuration.State\u0026#39; # If state is \u0026#34;Failed\u0026#34;, check StateReasonCode aws lambda get-function \\ --function-name EveryoneCook-dev-AuthModule \\ --query \u0026#39;Configuration.StateReasonCode\u0026#39; Issue: Function timeout\n# Increase timeout aws lambda update-function-configuration \\ --function-name EveryoneCook-dev-AuthModule \\ --timeout 30 Issue: Out of memory\n# Increase memory aws lambda update-function-configuration \\ --function-name EveryoneCook-dev-AuthModule \\ --memory-size 512 Issue: Environment variables missing\n# Update environment variables aws lambda update-function-configuration \\ --function-name EveryoneCook-dev-AuthModule \\ --environment Variables={TABLE_NAME=EveryoneCook-dev,REGION=us-east-1} Performance Monitoring Check Lambda metrics:\n# Invocations aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=EveryoneCook-dev-AuthModule \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Sum # Errors aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Errors \\ --dimensions Name=FunctionName,Value=EveryoneCook-dev-AuthModule \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Sum # Duration aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Duration \\ --dimensions Name=FunctionName,Value=EveryoneCook-dev-AuthModule \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average,Maximum Next Steps Once backend is deployed and verified, proceed to Test Endpoints End-to-End to test the complete application flow.\n"},{"uri":"https://hviethub.github.io/Internship-Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? Answer: Learn more about cloud and get enthusiastic support What do you think the company should improve for future interns? Answer: I don\u0026rsquo;t think there is anything that needs to be improved, if anything, it is about getting into the office (because there are too many people, so registering to go into the office is quite competitive) If recommending to a friend, would you suggest they intern here? Why or why not? Answer: Yes, because you get to practice in a corporate environment, interact with colleagues in the same industry, and participate in many activities and events to reinforce and learn more knowledge. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Answer: I had a very good experience during the internship. Would you like to continue this program in the future? Answer: Yes, if the time does not conflict with the school schedule Any other comments (free sharing): "},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.8-test-endpoints/","title":"Test Endpoints End-to-End","tags":[],"description":"","content":"Overview Sau khi deploy backend, bạn cần test tất cả endpoints để đảm bảo hệ thống hoạt động đúng từ đầu đến cuối.\nTesting Flow 1. Test Health Check 2. Test User Registration 3. Test Email Verification 4. Test User Login 5. Test Profile Management 6. Test Social Features (Posts, Comments, Likes) 7. Test Recipe Features 8. Test AI Features (Bedrock) 9. Test File Upload (S3 + CloudFront) 10. Test Search (OpenSearch) Step 1: Setup Testing Environment 1. Install Testing Tools\n# Install Postman (recommended) # Download from: https://www.postman.com/downloads/ # Or use curl for command-line testing 2. Set Environment Variables\n# Set API endpoint export API_ENDPOINT=\u0026#34;https://api.everyonecook.cloud\u0026#34; # Or for PowerShell $API_ENDPOINT = \u0026#34;https://api.everyonecook.cloud\u0026#34; Step 2: Test Health Check # Test health endpoint (no auth required) curl $API_ENDPOINT/health # Expected response: # { # \u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, # \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-01T00:00:00.000Z\u0026#34; # } Screenshot: Postman showing successful health check\nStep 3: Test User Registration 1. Register New User\n# Get User Pool Client ID CLIENT_ID=$(aws cloudformation describe-stacks \\ --stack-name EveryoneCook-dev-Auth \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`UserPoolClientId`].OutputValue\u0026#39; \\ --output text) # Register user aws cognito-idp sign-up \\ --client-id $CLIENT_ID \\ --username testuser \\ --password TestPassword123! \\ --user-attributes \\ Name=email,Value=test@example.com \\ Name=given_name,Value=\u0026#34;Test User\u0026#34; Expected: Success message with confirmation code sent to email\n2. Verify Pre-Signup Trigger\n# Check CloudWatch logs for Pre-Signup trigger aws logs tail /aws/lambda/EveryoneCook-dev-PreSignUpTrigger --since 5m Step 4: Test Email Verification 1. Get Verification Code\nCheck email for verification code (6 digits)\n2. Confirm User\n# Confirm user with code aws cognito-idp confirm-sign-up \\ --client-id $CLIENT_ID \\ --username testuser \\ --confirmation-code 123456 3. Verify Post-Confirmation Trigger\n# Check if user profile was created in DynamoDB aws dynamodb get-item \\ --table-name EveryoneCook-dev \\ --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;USER#testuser\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PROFILE\u0026#34;}}\u0026#39; # Should return user profile with: # - PK: USER#testuser # - SK: PROFILE # - userId: cognito-sub-id # - email: test@example.com # - fullName: Test User # - birthday: null # - gender: null # - country: null Screenshot: DynamoDB showing user profile created by trigger\nStep 5: Test User Login 1. Sign In\n# Sign in to get tokens TOKENS=$(aws cognito-idp initiate-auth \\ --client-id $CLIENT_ID \\ --auth-flow USER_PASSWORD_AUTH \\ --auth-parameters USERNAME=testuser,PASSWORD=TestPassword123!) # Extract tokens ACCESS_TOKEN=$(echo $TOKENS | jq -r \u0026#39;.AuthenticationResult.AccessToken\u0026#39;) ID_TOKEN=$(echo $TOKENS | jq -r \u0026#39;.AuthenticationResult.IdToken\u0026#39;) REFRESH_TOKEN=$(echo $TOKENS | jq -r \u0026#39;.AuthenticationResult.RefreshToken\u0026#39;) echo \u0026#34;ID Token: $ID_TOKEN\u0026#34; 2. Verify Post-Authentication Trigger\n# Check if lastLoginAt was updated aws dynamodb get-item \\ --table-name EveryoneCook-dev \\ --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;USER#testuser\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PROFILE\u0026#34;}}\u0026#39; \\ --projection-expression \u0026#34;lastLoginAt\u0026#34; Step 6: Test Profile Management 1. Get Profile\n# Get user profile curl -X GET \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ $API_ENDPOINT/auth/profile # Expected: User profile data 2. Update Profile\n# Update profile (onboarding) curl -X PUT \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;birthday\u0026#34;: \u0026#34;1990-01-01\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;US\u0026#34; }\u0026#39; \\ $API_ENDPOINT/auth/profile # Expected: Updated profile 3. Verify Update in DynamoDB\n# Check updated profile aws dynamodb get-item \\ --table-name EveryoneCook-dev \\ --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;USER#testuser\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PROFILE\u0026#34;}}\u0026#39; # Should show birthday, gender, country updated Screenshot: Postman showing profile update successful\nStep 7: Test Social Features 1. Create Post\n# Create a post POST_RESPONSE=$(curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;content\u0026#34;: \u0026#34;My first post on EveryoneCook!\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; }\u0026#39; \\ $API_ENDPOINT/social/posts) POST_ID=$(echo $POST_RESPONSE | jq -r \u0026#39;.postId\u0026#39;) echo \u0026#34;Created post: $POST_ID\u0026#34; 2. Get Posts Feed\n# Get posts curl -X GET \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ $API_ENDPOINT/social/posts # Expected: Array of posts including the one just created 3. Like Post\n# Like the post curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ $API_ENDPOINT/social/posts/$POST_ID/like # Expected: Success message 4. Comment on Post\n# Add comment curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;content\u0026#34;:\u0026#34;Great post!\u0026#34;}\u0026#39; \\ $API_ENDPOINT/social/posts/$POST_ID/comment # Expected: Comment created Screenshot: Postman showing post creation and interactions\nStep 8: Test Recipe Features 1. Create Recipe\n# Create a recipe RECIPE_RESPONSE=$(curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Pho Bo (Vietnamese Beef Noodle Soup)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Traditional Vietnamese beef noodle soup\u0026#34;, \u0026#34;ingredients\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;beef bones\u0026#34;, \u0026#34;amount\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;kg\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;rice noodles\u0026#34;, \u0026#34;amount\u0026#34;: \u0026#34;500\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;g\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;ginger\u0026#34;, \u0026#34;amount\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;piece\u0026#34;} ], \u0026#34;instructions\u0026#34;: [ \u0026#34;Boil beef bones for 2 hours\u0026#34;, \u0026#34;Add spices and simmer\u0026#34;, \u0026#34;Prepare noodles and serve\u0026#34; ], \u0026#34;cuisine\u0026#34;: \u0026#34;Vietnamese\u0026#34;, \u0026#34;difficulty\u0026#34;: \u0026#34;medium\u0026#34;, \u0026#34;prepTime\u0026#34;: 30, \u0026#34;cookTime\u0026#34;: 120 }\u0026#39; \\ $API_ENDPOINT/recipes) RECIPE_ID=$(echo $RECIPE_RESPONSE | jq -r \u0026#39;.recipeId\u0026#39;) echo \u0026#34;Created recipe: $RECIPE_ID\u0026#34; 2. Get Recipes\n# Get all recipes curl -X GET \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ $API_ENDPOINT/recipes # Expected: Array of recipes 3. Get Recipe by ID\n# Get specific recipe curl -X GET \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ $API_ENDPOINT/recipes/$RECIPE_ID # Expected: Recipe details Step 9: Test AI Features 1. Generate Recipe with AI\n# Generate recipe using Bedrock curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;ingredients\u0026#34;: [\u0026#34;chicken\u0026#34;, \u0026#34;rice\u0026#34;, \u0026#34;vegetables\u0026#34;], \u0026#34;cuisine\u0026#34;: \u0026#34;Vietnamese\u0026#34;, \u0026#34;dietaryRestrictions\u0026#34;: [\u0026#34;gluten-free\u0026#34;], \u0026#34;servings\u0026#34;: 4 }\u0026#39; \\ $API_ENDPOINT/ai/generate-recipe # Expected: AI-generated recipe (takes 5-10 seconds) # Response includes Vietnamese ingredient names Screenshot: Postman showing AI-generated recipe with Vietnamese names\n2. Translate Ingredient\n# Translate ingredient to Vietnamese curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;ingredient\u0026#34;: \u0026#34;tomato\u0026#34;, \u0026#34;targetLanguage\u0026#34;: \u0026#34;vi\u0026#34; }\u0026#39; \\ $API_ENDPOINT/ai/translate # Expected: {\u0026#34;translation\u0026#34;: \u0026#34;cà chua\u0026#34;, \u0026#34;confidence\u0026#34;: 0.99} Step 10: Test File Upload 1. Request Pre-signed URL\n# Get pre-signed URL for avatar upload UPLOAD_RESPONSE=$(curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;fileType\u0026#34;: \u0026#34;avatar\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;avatar.jpg\u0026#34;, \u0026#34;contentType\u0026#34;: \u0026#34;image/jpeg\u0026#34;, \u0026#34;fileSize\u0026#34;: 1024000 }\u0026#39; \\ $API_ENDPOINT/upload/presigned-url) PRESIGNED_URL=$(echo $UPLOAD_RESPONSE | jq -r \u0026#39;.url\u0026#39;) UPLOAD_KEY=$(echo $UPLOAD_RESPONSE | jq -r \u0026#39;.key\u0026#39;) echo \u0026#34;Pre-signed URL: $PRESIGNED_URL\u0026#34; echo \u0026#34;Upload Key: $UPLOAD_KEY\u0026#34; 2. Upload File to S3\n# Create test image echo \u0026#34;Test image content\u0026#34; \u0026gt; test-avatar.jpg # Upload using pre-signed URL curl -X PUT \\ -H \u0026#34;Content-Type: image/jpeg\u0026#34; \\ --upload-file test-avatar.jpg \\ \u0026#34;$PRESIGNED_URL\u0026#34; # Expected: 200 OK 3. Mark Upload Complete\n# Notify backend that upload is complete curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;key\\\u0026#34;:\\\u0026#34;$UPLOAD_KEY\\\u0026#34;}\u0026#34; \\ $API_ENDPOINT/upload/complete # Expected: Success message 4. Access via CloudFront\n# Access file via CloudFront CDN curl -I https://cdn.everyonecook.cloud/$UPLOAD_KEY # First request: X-Cache: Miss from cloudfront # Second request: X-Cache: Hit from cloudfront Screenshot: CloudFront cache headers showing Miss then Hit\nStep 11: Test Search (OpenSearch) If OpenSearch is enabled:\n# Search recipes with Vietnamese query curl -X POST \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;query\u0026#34;: \u0026#34;phở bò\u0026#34;, \u0026#34;filters\u0026#34;: { \u0026#34;cuisine\u0026#34;: \u0026#34;Vietnamese\u0026#34;, \u0026#34;difficulty\u0026#34;: \u0026#34;medium\u0026#34; }, \u0026#34;limit\u0026#34;: 10 }\u0026#39; \\ $API_ENDPOINT/ai/search # Expected: Array of matching recipes # Vietnamese analyzer handles: \u0026#34;phở bò\u0026#34; = \u0026#34;pho bo\u0026#34; = \u0026#34;beef noodle soup\u0026#34; Step 12: Test Admin Features 1. List Users (Admin Only)\n# Get all users (requires admin role) curl -X GET \\ -H \u0026#34;Authorization: Bearer $ID_TOKEN\u0026#34; \\ $API_ENDPOINT/admin/users # Expected: Array of users or 403 Forbidden if not admin Step 13: Verify Async Processing 1. Check SQS Queue Processing\n# Send message to SearchIndex queue QUEUE_URL=$(aws sqs list-queues \\ --queue-name-prefix EveryoneCook-dev-SearchIndexQueue \\ | jq -r \u0026#39;.QueueUrls[0]\u0026#39;) aws sqs send-message \\ --queue-url $QUEUE_URL \\ --message-body \u0026#34;{ \\\u0026#34;eventName\\\u0026#34;: \\\u0026#34;INSERT\\\u0026#34;, \\\u0026#34;tableName\\\u0026#34;: \\\u0026#34;recipes\\\u0026#34;, \\\u0026#34;keys\\\u0026#34;: {\\\u0026#34;PK\\\u0026#34;: \\\u0026#34;USER#testuser\\\u0026#34;, \\\u0026#34;SK\\\u0026#34;: \\\u0026#34;RECIPE#$RECIPE_ID\\\u0026#34;}, \\\u0026#34;newImage\\\u0026#34;: { \\\u0026#34;title\\\u0026#34;: \\\u0026#34;Pho Bo\\\u0026#34;, \\\u0026#34;ingredients\\\u0026#34;: [\\\u0026#34;beef\\\u0026#34;, \\\u0026#34;noodles\\\u0026#34;], \\\u0026#34;cuisine\\\u0026#34;: \\\u0026#34;Vietnamese\\\u0026#34; } }\u0026#34; # Check worker logs aws logs tail /aws/lambda/EveryoneCook-dev-SearchSyncWorker --follow Testing Checklist Health check responds User registration works Email verification works User login successful Profile CRUD operations work Posts can be created Posts can be liked Comments can be added Recipes can be created AI recipe generation works Ingredient translation works File upload to S3 works CloudFront serves files Search works (if OpenSearch enabled) SQS queues process messages All Lambda functions execute without errors Performance Benchmarks Expected Response Times:\nHealth check: \u0026lt; 50ms User login: \u0026lt; 200ms Get profile: \u0026lt; 100ms Create post: \u0026lt; 300ms Get posts feed: \u0026lt; 500ms AI recipe generation: 5-10 seconds File upload (pre-signed URL): \u0026lt; 100ms Search query: \u0026lt; 200ms Next Steps Once all tests pass, proceed to Push to GitLab to version control your code and set up CI/CD.\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.9-push-gitlab/","title":"Push to GitLab","tags":[],"description":"","content":"Overview Sau khi test thành công, bạn sẽ push code lên GitLab để version control và chuẩn bị cho CI/CD.\nStep 1: Initialize Git Repository 1. Check Git Status\n# Navigate to project root cd /path/to/everyonecook-dev # Check if Git is already initialized git status # If not initialized: git init 2. Configure Git\n# Set your name and email git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your.email@example.com\u0026#34; # Verify configuration git config --list Step 2: Create .gitignore 1. Create .gitignore File\n# Create .gitignore cat \u0026gt; .gitignore \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Dependencies node_modules/ package-lock.json yarn.lock # Build outputs dist/ build/ *.js.map *.d.ts.map cdk.out/ .next/ # Environment variables .env .env.local .env.*.local # AWS *.pem *.key cloudfront-private-key.pem # Deployment packages *.zip deployment/ # Logs logs/ *.log npm-debug.log* yarn-debug.log* yarn-error.log* # IDE .vscode/ .idea/ *.swp *.swo *~ # OS .DS_Store Thumbs.db # Test coverage coverage/ .nyc_output/ # Temporary files tmp/ temp/ *.tmp # CDK cdk.context.json outputs.json # TypeScript *.tsbuildinfo EOF Screenshot: .gitignore file content\nStep 3: Stage and Commit Files 1. Add Files to Staging\n# Add all files git add . # Check what will be committed git status 2. Create Initial Commit\n# Commit with message git commit -m \u0026#34;Initial commit: EveryoneCook infrastructure and backend\u0026#34; # Verify commit git log --oneline Screenshot: Terminal showing initial commit\nStep 4: Create GitLab Repository 1. Login to GitLab\nGo to https://gitlab.com/ and login\n2. Create New Project\nClick \u0026ldquo;New project\u0026rdquo; Choose \u0026ldquo;Create blank project\u0026rdquo; Project name: everyonecook Visibility: Private (recommended) Initialize with README: No (we already have code) Click \u0026ldquo;Create project\u0026rdquo; Screenshot: GitLab showing new project created\n3. Get Repository URL\n# HTTPS URL https://gitlab.com/your-username/everyonecook.git # SSH URL (if you have SSH key configured) git@gitlab.com:your-username/everyonecook.git Step 5: Add Remote and Push 1. Add GitLab Remote\n# Add remote (use HTTPS) git remote add origin https://gitlab.com/your-username/everyonecook.git # Or use SSH git remote add origin git@gitlab.com:your-username/everyonecook.git # Verify remote git remote -v 2. Push to GitLab\n# Push to main branch git push -u origin main # Enter GitLab credentials if using HTTPS # Username: your-username # Password: your-personal-access-token (not your password!) Screenshot: Terminal showing successful push to GitLab\n3. Verify on GitLab\nGo to your GitLab repository and verify files are uploaded.\nScreenshot: GitLab showing repository with files\nStep 6: Create Branch Structure 1. Create Development Branch\n# Create and switch to dev branch git checkout -b dev # Push dev branch git push -u origin dev 2. Create Feature Branch\n# Create feature branch from dev git checkout -b feature/add-notifications # Make changes... # Commit changes... # Push feature branch git push -u origin feature/add-notifications 3. Branch Strategy\nmain (production) ↓ dev (development) ↓ feature/* (features) hotfix/* (urgent fixes) Step 7: Configure GitLab CI/CD 1. Create .gitlab-ci.yml\n# Create CI/CD configuration cat \u0026gt; .gitlab-ci.yml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; image: node:20 stages: - test - build - deploy variables: AWS_DEFAULT_REGION: us-east-1 # Cache node_modules cache: paths: - node_modules/ - infrastructure/node_modules/ - services/*/node_modules/ # Test stage test: stage: test script: - npm install - npm run lint - npm run test only: - merge_requests - dev - main # Build stage build: stage: build script: - npm install - npm run build - cd infrastructure \u0026amp;\u0026amp; npm run build artifacts: paths: - infrastructure/cdk.out/ - services/*/dist/ expire_in: 1 hour only: - dev - main # Deploy to dev deploy:dev: stage: deploy script: - npm install -g aws-cdk - cd infrastructure - npx cdk deploy --all --require-approval never --context environment=dev environment: name: development url: https://api.everyonecook.cloud only: - dev when: manual # Deploy to production deploy:prod: stage: deploy script: - npm install -g aws-cdk - cd infrastructure - npx cdk deploy --all --require-approval never --context environment=prod environment: name: production url: https://api.everyonecook.cloud only: - main when: manual EOF 2. Commit CI/CD Configuration\n# Add and commit git add .gitlab-ci.yml git commit -m \u0026#34;Add GitLab CI/CD configuration\u0026#34; git push Screenshot: GitLab showing CI/CD pipeline\nStep 8: Configure GitLab Variables 1. Add AWS Credentials\nGo to GitLab → Settings → CI/CD → Variables Add variables: AWS_ACCESS_KEY_ID: Your AWS access key AWS_SECRET_ACCESS_KEY: Your AWS secret key (masked) AWS_DEFAULT_REGION: us-east-1 Screenshot: GitLab CI/CD variables configuration\n2. Add Environment Variables\nAdd other environment variables:\nDOMAIN_NAME: everyonecook.cloud ENABLE_OPENSEARCH: false etc. Step 9: Set Up Branch Protection 1. Protect Main Branch\nGo to Settings → Repository → Protected branches Select main branch Allowed to merge: Maintainers Allowed to push: No one Require approval: Yes (1 approval) 2. Protect Dev Branch\nSelect dev branch Allowed to merge: Developers + Maintainers Allowed to push: Developers + Maintainers Step 10: Create Merge Request Workflow 1. Create Feature Branch\n# Create feature branch git checkout dev git pull git checkout -b feature/new-feature # Make changes # ... # Commit changes git add . git commit -m \u0026#34;Add new feature\u0026#34; git push -u origin feature/new-feature 2. Create Merge Request\nGo to GitLab → Merge requests → New merge request Source branch: feature/new-feature Target branch: dev Title: \u0026ldquo;Add new feature\u0026rdquo; Description: Describe changes Assign to reviewer Create merge request 3. Review and Merge\nReviewer reviews code CI/CD pipeline runs tests If tests pass and approved, merge to dev Delete feature branch Step 11: Tag Releases 1. Create Release Tag\n# After merging to main git checkout main git pull # Create tag git tag -a v1.0.0 -m \u0026#34;Release version 1.0.0\u0026#34; # Push tag git push origin v1.0.0 2. Create Release on GitLab\nGo to Repository → Tags Find your tag Click \u0026ldquo;Create release\u0026rdquo; Add release notes Publish release Git Workflow Summary 1. Create feature branch from dev git checkout -b feature/new-feature 2. Make changes and commit git add . git commit -m \u0026#34;Description\u0026#34; 3. Push to GitLab git push -u origin feature/new-feature 4. Create merge request on GitLab feature/new-feature → dev 5. Review, test, and merge 6. Deploy to dev environment (manual trigger in GitLab CI/CD) 7. Test in dev environment 8. Merge dev → main for production 9. Deploy to production (manual trigger in GitLab CI/CD) 10. Tag release git tag -a v1.0.0 -m \u0026#34;Release\u0026#34; Best Practices Commit Messages:\nfeat: Add user authentication fix: Fix login bug docs: Update README style: Format code refactor: Refactor user service test: Add unit tests chore: Update dependencies Branch Naming:\nfeature/add-notifications bugfix/fix-login-error hotfix/critical-security-patch release/v1.0.0 Troubleshooting Issue: Authentication failed\n# Use personal access token instead of password # Generate token: GitLab → Settings → Access Tokens # Use token as password when pushing Issue: Large files rejected\n# Check file size git ls-files -z | xargs -0 du -h | sort -h # Remove large files from history git filter-branch --tree-filter \u0026#39;rm -f large-file.zip\u0026#39; HEAD Issue: Merge conflicts\n# Update your branch git checkout feature/your-feature git fetch origin git merge origin/dev # Resolve conflicts in files # Then commit git add . git commit -m \u0026#34;Resolve merge conflicts\u0026#34; git push Next Steps Once code is pushed to GitLab, proceed to Deploy to Amplify to deploy your frontend application.\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.10-deploy-amplify/","title":"Deploy to Amplify","tags":[],"description":"","content":"Overview Bước cuối cùng là deploy frontend Next.js 15 lên AWS Amplify với tích hợp GitLab để tự động deploy khi có code mới.\nStep 1: Prepare Frontend Code 1. Check Frontend Structure\ncd frontend # Check package.json cat package.json # Should show Next.js 15 2. Test Frontend Locally\n# Install dependencies npm install # Run development server npm run dev # Open http://localhost:3000 3. Build Frontend\n# Build for production npm run build # Test production build npm start Step 2: Create Amplify App 1. Go to AWS Amplify Console\nOpen AWS Console Search for \u0026ldquo;Amplify\u0026rdquo; Click \u0026ldquo;Get started\u0026rdquo; or \u0026ldquo;New app\u0026rdquo; 2. Connect to GitLab\nChoose \u0026ldquo;Host web app\u0026rdquo; Select \u0026ldquo;GitLab\u0026rdquo; Click \u0026ldquo;Connect branch\u0026rdquo; Authorize AWS Amplify to access GitLab Screenshot: Amplify showing GitLab connection\n3. Select Repository\nChoose repository: everyonecook Choose branch: main (or dev for development) Click \u0026ldquo;Next\u0026rdquo; Step 3: Configure Build Settings 1. App Name\nApp name: everyonecook-frontend 2. Build Settings\nAmplify auto-detects Next.js, but verify:\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: .next files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* - .next/cache/**/* Screenshot: Amplify build settings for Next.js\n3. Advanced Settings\nAdd environment variables:\nNEXT_PUBLIC_API_URL=https://api.everyonecook.cloud NEXT_PUBLIC_CDN_URL=https://cdn.everyonecook.cloud NEXT_PUBLIC_USER_POOL_ID=us-east-1_ABC123 NEXT_PUBLIC_USER_POOL_CLIENT_ID=abc123def456 NEXT_PUBLIC_REGION=us-east-1 Screenshot: Amplify environment variables configuration\nStep 4: Deploy Frontend 1. Start Deployment\nClick \u0026ldquo;Save and deploy\u0026rdquo;\nAmplify will:\nClone repository from GitLab Install dependencies Build Next.js app Deploy to CDN Provision domain 2. Monitor Deployment\nWatch deployment progress:\nProvision Build Deploy Verify Screenshot: Amplify showing deployment in progress\n3. Wait for Completion\nDeployment takes 5-10 minutes.\nScreenshot: Amplify showing successful deployment\nStep 5: Configure Custom Domain 1. Add Custom Domain\nGo to Amplify app → Domain management Click \u0026ldquo;Add domain\u0026rdquo; Enter domain: everyonecook.cloud Amplify will auto-configure: Root domain: everyonecook.cloud WWW subdomain: www.everyonecook.cloud 2. DNS Configuration\nAmplify automatically creates DNS records in Route 53:\neveryonecook.cloud → A record → Amplify www.everyonecook.cloud → CNAME → Amplify 3. SSL Certificate\nAmplify automatically provisions SSL certificate via ACM.\n4. Wait for DNS Propagation\nTakes 5-15 minutes.\nScreenshot: Amplify showing custom domain configured\nStep 6: Verify Deployment 1. Access Frontend\n# Via Amplify domain curl -I https://main.d1234567890.amplifyapp.com # Via custom domain curl -I https://everyonecook.cloud # Should return 200 OK 2. Test Frontend Features\nOpen https://everyonecook.cloud in browser Test user registration Test login Test creating posts Test creating recipes Test AI features Screenshot: Browser showing EveryoneCook frontend live\nStep 7: Configure Auto-Deploy 1. Enable Auto-Deploy\nAuto-deploy is enabled by default. When you push to GitLab:\ngit push origin main Amplify automatically:\nDetects new commit Starts build Deploys new version Updates live site 2. Configure Branch Deployments\nSet up multiple environments:\nMain branch → Production (everyonecook.cloud) Dev branch → Development (dev.everyonecook.cloud) Add dev branch:\nGo to Amplify → App settings → General Click \u0026ldquo;Connect branch\u0026rdquo; Select dev branch Configure subdomain: dev.everyonecook.cloud Screenshot: Amplify showing multiple branch deployments\nStep 8: Configure Build Notifications 1. Add SNS Notifications\nGo to Amplify → Notifications Click \u0026ldquo;Add notification\u0026rdquo; Select events: Build started Build succeeded Build failed Enter email address Save 2. Slack Integration (Optional)\nCreate Slack webhook Go to Amplify → Notifications Add webhook URL Test notification Step 9: Monitor Performance 1. Check Amplify Metrics\nGo to Amplify → Monitoring View metrics: Requests Data transfer Errors Latency 2. Enable CloudWatch Logs\n# View Amplify build logs aws amplify get-job \\ --app-id d1234567890 \\ --branch-name main \\ --job-id 1 Step 10: Configure Redirects and Rewrites 1. Add Redirects\nCreate amplify.yml in frontend root:\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: .next files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* - .next/cache/**/* customHeaders: - pattern: \u0026#39;**/*\u0026#39; headers: - key: \u0026#39;Strict-Transport-Security\u0026#39; value: \u0026#39;max-age=31536000; includeSubDomains\u0026#39; - key: \u0026#39;X-Frame-Options\u0026#39; value: \u0026#39;SAMEORIGIN\u0026#39; - key: \u0026#39;X-Content-Type-Options\u0026#39; value: \u0026#39;nosniff\u0026#39; redirects: - source: \u0026#39;/\u0026lt;*\u0026gt;\u0026#39; target: \u0026#39;/index.html\u0026#39; status: \u0026#39;404-200\u0026#39; 2. Commit and Push\ngit add amplify.yml git commit -m \u0026#34;Add Amplify configuration\u0026#34; git push origin main Deployment Summary Deployed Resources:\n✅ Amplify App ✅ Frontend hosted on Amplify CDN ✅ Custom domain configured ✅ SSL certificate provisioned ✅ Auto-deploy from GitLab enabled ✅ Multiple branch deployments ✅ Build notifications configured URLs:\nProduction: https://everyonecook.cloud Development: https://dev.everyonecook.cloud (if configured) Amplify: https://main.d1234567890.amplifyapp.com Cost Amplify Hosting:\nBuild minutes: First 1,000 minutes/month free Hosting: First 15 GB served/month free After free tier: $0.01 per build minute, $0.15 per GB served Estimated Cost:\nDev environment: $0-5/month Production (low traffic): $5-20/month Production (high traffic): $20-100/month Troubleshooting Issue: Build fails\n# Check build logs in Amplify console # Common issues: # - Missing environment variables # - Node version mismatch # - Build command incorrect Issue: Custom domain not working\n# Check DNS propagation dig everyonecook.cloud # Check Route 53 records aws route53 list-resource-record-sets \\ --hosted-zone-id YOUR-ZONE-ID Issue: Environment variables not working\n# Ensure variables start with NEXT_PUBLIC_ for client-side # Redeploy after adding variables Best Practices Use Environment Variables: Never hardcode API URLs Enable Auto-Deploy: Automatic deployments on push Multiple Environments: Separate dev and prod Monitor Performance: Use Amplify metrics Set Up Notifications: Get alerted on build failures Use Custom Domain: Professional appearance Enable HTTPS: Always use SSL Configure Headers: Security headers for protection Next Steps Congratulations! Your application is now fully deployed:\n✅ Infrastructure on AWS ✅ Backend APIs running ✅ Frontend on Amplify ✅ Code on GitLab ✅ CI/CD configured Proceed to Cleanup when you\u0026rsquo;re done testing, or start using your application!\n"},{"uri":"https://hviethub.github.io/Internship-Report/5-workshop/5.11-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Overview Khi bạn hoàn thành workshop và không muốn tiếp tục sử dụng, hãy xóa tất cả resources để tránh phí phát sinh.\n⚠️ Warning: Quá trình này không thể hoàn tác. Tất cả dữ liệu sẽ bị xóa vĩnh viễn.\nCleanup Order Phải xóa theo thứ tự ngược lại với deployment:\n1. Amplify App (Frontend) 2. Observability Stack 3. Backend Stack 4. Auth Stack 5. Core Stack 6. Certificate Stack 7. DNS Stack 8. CDK Bootstrap (optional) Step 1: Delete Amplify App 1. Delete via Console\nGo to AWS Amplify Console Select your app Click \u0026ldquo;Actions\u0026rdquo; → \u0026ldquo;Delete app\u0026rdquo; Type app name to confirm Click \u0026ldquo;Delete\u0026rdquo; 2. Delete via CLI\n# Get app ID APP_ID=$(aws amplify list-apps \\ --query \u0026#39;apps[?name==`everyonecook-frontend`].appId\u0026#39; \\ --output text) # Delete app aws amplify delete-app --app-id $APP_ID Step 2: Empty S3 Buckets S3 buckets phải empty trước khi xóa:\n# List all EveryoneCook buckets aws s3 ls | grep everyonecook # Empty each bucket (4 buckets) aws s3 rm s3://everyonecook-content-dev-xxxxx --recursive aws s3 rm s3://everyonecook-logs-dev-xxxxx --recursive aws s3 rm s3://everyonecook-emails-dev-xxxxx --recursive aws s3 rm s3://everyonecook-cdn-logs-dev-xxxxx --recursive # Or use PowerShell script $buckets = aws s3 ls | Select-String \u0026#34;everyonecook\u0026#34; | ForEach-Object { $_.ToString().Split()[-1] } foreach ($bucket in $buckets) { Write-Host \u0026#34;Emptying bucket: $bucket\u0026#34; aws s3 rm \u0026#34;s3://$bucket\u0026#34; --recursive } Step 3: Delete CDK Stacks 1. Delete Observability Stack\ncd infrastructure # Delete Observability stack npx cdk destroy EveryoneCook-dev-Observability --context environment=dev # Type \u0026#39;y\u0026#39; to confirm 2. Delete Backend Stack\n# Delete Backend stack npx cdk destroy EveryoneCook-dev-Backend --context environment=dev # Type \u0026#39;y\u0026#39; to confirm 3. Delete Auth Stack\n# Delete Auth stack npx cdk destroy EveryoneCook-dev-Auth --context environment=dev # Type \u0026#39;y\u0026#39; to confirm 4. Delete Core Stack\n# Delete Core stack (takes 15-20 minutes due to CloudFront) npx cdk destroy EveryoneCook-dev-Core --context environment=dev # Type \u0026#39;y\u0026#39; to confirm 5. Delete Certificate Stack\n# Delete Certificate stack npx cdk destroy EveryoneCook-dev-Certificate --context environment=dev # Type \u0026#39;y\u0026#39; to confirm 6. Delete DNS Stack (Optional)\n# Delete DNS stack (only if you don\u0026#39;t need the domain) npx cdk destroy EveryoneCook-dev-DNS --context environment=dev # Type \u0026#39;y\u0026#39; to confirm Step 4: Verify All Stacks Deleted # List remaining stacks aws cloudformation list-stacks \\ --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE \\ --query \u0026#39;StackSummaries[?contains(StackName, `EveryoneCook-dev`)].StackName\u0026#39; # Should return empty array Step 5: Delete CDK Bootstrap (Optional) ⚠️ Only do this if you\u0026rsquo;re done with CDK completely:\n# Delete CDK bootstrap stack aws cloudformation delete-stack --stack-name CDKToolkit # Empty and delete CDK assets bucket BUCKET_NAME=$(aws s3 ls | grep cdk | awk \u0026#39;{print $3}\u0026#39;) aws s3 rm s3://$BUCKET_NAME --recursive aws s3 rb s3://$BUCKET_NAME Step 6: Delete GitLab Repository (Optional) 1. Archive Repository\nGo to GitLab → Settings → General Scroll to \u0026ldquo;Advanced\u0026rdquo; Click \u0026ldquo;Archive project\u0026rdquo; 2. Delete Repository\nGo to GitLab → Settings → General Scroll to \u0026ldquo;Advanced\u0026rdquo; Click \u0026ldquo;Delete project\u0026rdquo; Type project name to confirm Click \u0026ldquo;Yes, delete project\u0026rdquo; Step 7: Verify Complete Cleanup 1. Check CloudFormation\naws cloudformation list-stacks \\ --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE \\ | grep EveryoneCook # Should return nothing 2. Check S3\naws s3 ls | grep everyonecook # Should return nothing 3. Check Lambda\naws lambda list-functions | grep EveryoneCook # Should return nothing 4. Check DynamoDB\naws dynamodb list-tables | grep EveryoneCook # Should return nothing 5. Check API Gateway\naws apigateway get-rest-apis | grep EveryoneCook # Should return nothing 6. Check Cognito\naws cognito-idp list-user-pools --max-results 10 | grep EveryoneCook # Should return nothing 7. Check CloudFront\naws cloudfront list-distributions | grep EveryoneCook # Should return nothing 8. Check OpenSearch\naws opensearch list-domain-names | grep everyonecook # Should return nothing 9. Check Amplify\naws amplify list-apps | grep everyonecook # Should return nothing Cost After Cleanup Immediate:\nMost resources: $0/month Route 53 Hosted Zone: $0.50/month (if kept) KMS keys: Scheduled for deletion (7-30 days), no charge during waiting period After 30 days:\nEverything: $0/month (if DNS stack also deleted) Troubleshooting Cleanup Issue: S3 bucket deletion fails\n# Force empty and delete aws s3 rb s3://bucket-name --force Issue: CloudFormation stack stuck\n# Check stack events aws cloudformation describe-stack-events \\ --stack-name EveryoneCook-dev-Core \\ --max-items 20 # If stuck, skip failed resources aws cloudformation delete-stack \\ --stack-name EveryoneCook-dev-Core \\ --retain-resources ResourceLogicalId Issue: CloudFront distribution can\u0026rsquo;t be deleted\n# Disable distribution first DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;EveryoneCook-dev\u0026#39;].Id\u0026#34; \\ --output text) # Get ETag ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#34;ETag\u0026#34; --output text) # Disable distribution aws cloudfront update-distribution \\ --id $DIST_ID \\ --if-match $ETAG \\ --distribution-config file://disabled-config.json # Wait 15-20 minutes, then delete aws cloudfront delete-distribution --id $DIST_ID --if-match $ETAG Issue: DynamoDB table has deletion protection\n# Disable deletion protection aws dynamodb update-table \\ --table-name EveryoneCook-dev \\ --no-deletion-protection-enabled # Then delete stack Final Verification # Check AWS billing aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost # Should show decreasing costs Cleanup Checklist Amplify app deleted All S3 buckets emptied and deleted Observability stack deleted Backend stack deleted Auth stack deleted Core stack deleted Certificate stack deleted DNS stack deleted (optional) CDK bootstrap deleted (optional) GitLab repository archived/deleted (optional) No remaining CloudFormation stacks No remaining Lambda functions No remaining DynamoDB tables No remaining S3 buckets No remaining CloudFront distributions No remaining Cognito user pools Billing shows $0 or minimal cost Conclusion Bạn đã hoàn thành workshop EveryoneCook! Bạn đã học được:\n✅ Infrastructure as Code với AWS CDK ✅ Serverless Architecture với Lambda và API Gateway ✅ DynamoDB Single Table Design với username-based PK ✅ OpenSearch với Vietnamese analyzer ✅ CloudFront CDN với Origin Access Control ✅ Cognito Authentication với Lambda triggers ✅ Bedrock AI integration ✅ GitLab version control và CI/CD ✅ AWS Amplify frontend deployment ✅ CloudWatch monitoring và X-Ray tracing\nTổng cộng đã deploy:\n7 CDK stacks 100+ AWS resources 6 Lambda modules + 1 worker Full-stack application CI/CD pipeline Production-ready infrastructure Cảm ơn bạn đã hoàn thành workshop! 🎉\n"},{"uri":"https://hviethub.github.io/Internship-Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://hviethub.github.io/Internship-Report/tags/","title":"Tags","tags":[],"description":"","content":""}]